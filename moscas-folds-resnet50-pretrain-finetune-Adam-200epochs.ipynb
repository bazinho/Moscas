{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import glob\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold                                                                                                                       \n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn_evaluation.plot as skplot\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as colormap\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,Flatten,Dense,Dropout,GlobalAveragePooling2D,Conv2D,MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imagedir = \"Datasets/Original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\tClass:     fraterculus\tNumber of images: 100\n",
      "Label: 1\tClass:         obliqua\tNumber of images: 101\n",
      "Label: 2\tClass:       sororcula\tNumber of images: 100\n",
      "Processing images ...\n",
      "Images processed: 301\n"
     ]
    }
   ],
   "source": [
    "cur_dir = os.getcwd()\n",
    "os.chdir(imagedir)  # the parent folder with sub-folders\n",
    "\n",
    "# Get number of samples per family\n",
    "list_fams = sorted(os.listdir(os.getcwd()), key=str.lower)  # vector of strings with family names\n",
    "no_imgs = []  # No. of samples per family\n",
    "for i in range(len(list_fams)):\n",
    "    os.chdir(list_fams[i])\n",
    "    len1 = len(glob.glob('*.ppm'))  # assuming the images are stored as 'ppm'\n",
    "    no_imgs.append(len1)\n",
    "    os.chdir('..')\n",
    "num_samples = np.sum(no_imgs)  # total number of all samples\n",
    "\n",
    "# Compute the labels\n",
    "y = np.zeros(num_samples)\n",
    "pos = 0\n",
    "label = 0\n",
    "for i in no_imgs:\n",
    "    print (\"Label:%2d\\tClass: %15s\\tNumber of images: %d\" % (label, list_fams[label], i))\n",
    "    for j in range(i):\n",
    "        y[pos] = label\n",
    "        pos += 1\n",
    "    label += 1\n",
    "num_classes = label\n",
    "\n",
    "# Compute the features\n",
    "width, height,channels = (224,224,3)\n",
    "X = np.zeros((num_samples, width, height, channels))\n",
    "cnt = 0\n",
    "list_paths = [] # List of image paths\n",
    "print(\"Processing images ...\")\n",
    "for i in range(len(list_fams)):\n",
    "    for img_file in glob.glob(list_fams[i]+'/*.ppm'):\n",
    "        #print(\"[%d] Processing image: %s\" % (cnt, img_file))\n",
    "        list_paths.append(os.path.join(os.getcwd(),img_file))\n",
    "        img = image.load_img(img_file, target_size=(224, 224))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        X[cnt] = x\n",
    "        cnt += 1\n",
    "print(\"Images processed: %d\" %(cnt))\n",
    "\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding classes (y) into integers (y_encoded) and then generating one-hot-encoding (Y)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y_encoded = encoder.transform(y)\n",
    "Y = np_utils.to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F035-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F025-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F084-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F062-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F021-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F042-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F098-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F115-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F048-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F010-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F022-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F006-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F055-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F110-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F020-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F094-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F011-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F004-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F090-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F026-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F028-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F052-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F103-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F072-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F007-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F043-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F031-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F083-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F102-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F089-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F033-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F078-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F014-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F027-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F087-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F107-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F053-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F040-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F071-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F037-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F088-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F063-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F080-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F075-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F093-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F036-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F059-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F132-N-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F104-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F005-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F046-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F079-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F100-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F108-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F003-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F039-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F030-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F076-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F109-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F066-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F111-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F112-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F113-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F095-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F047-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F101-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F009-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F041-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F016,1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F086-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F023-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F097-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F099-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F051-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F045-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F105-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F049-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F050-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F092-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F044-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F058-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F060-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F114-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F077-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F085-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F057-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F068-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F064-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F061-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F029-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F012-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F034-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F017-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F082-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F091-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F013-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F054-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F056-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F015-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/fraterculus/F106-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B089-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B027-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B069-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B107-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B014-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B058-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B092-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B079-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B081-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B054-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B032-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B065-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B096-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B094-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B005-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B093-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B050-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B059-M-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B087-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B097-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B047-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B066-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B080-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B006-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B095-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B068-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B082-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B091-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B026-15x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B055-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B049-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B001-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B100-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B085-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B051-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B086-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B034-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B038-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B075-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B033-15,x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B009-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B002-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B106-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B028-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B048-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B083-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B101-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B016-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B090-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B104-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B070-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B103-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B077-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B060-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B039-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B061-M-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B076-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B102-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B024-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B008-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B056-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B063-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B019-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B045-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B017-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B031-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B062-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B012-1,5X.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B065-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B043-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B067-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B040-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B078-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B071-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B084-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B108-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B074-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B105_G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B041-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B004-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B015-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B046-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B042-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B025-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B010-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B052-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B030-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B073-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B099-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B020-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B013-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B098-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B044-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B003,1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B057-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B018-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B072-G-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B053-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B029-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B007-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/obliqua/B088-G01,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S033-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S041-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S016-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S038-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S011-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S043-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S099-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S116-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S100-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S059-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S021-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S056-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S096-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S036-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S075-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S094-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S069-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S027-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S042-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S082-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S034-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S074-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S071-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S024-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S045-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S010-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S064-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S015-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S111-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S040-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S088-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S012-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S078-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S123-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S073-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S026-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S076-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S067-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S053-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S018-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S054-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S019-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S029-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S109-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S003-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S063-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S112-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S062-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S058-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S005-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S105-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S081-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S035-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S093-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S048-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S039-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S004-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S055-1,5.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S101-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S061-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S118-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S001-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S060-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S046-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S080-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S025-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S106-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S103-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S092-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S079-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S022-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S068-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S083-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S009-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S087-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S037-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S072-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S104-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S098-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S051-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S108-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S020-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S091-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S031-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S007-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S032-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S065-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S095-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S030-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S008-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S006-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S049-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S050-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S023-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S090-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S057-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S052-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S044-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/S028-1,5x.ppm',\n",
       " '/home/edmar/GIT/Moscas/Datasets/Original/sororcula/s102-1,5x.ppm']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/Folds/Fold-Train1.txt\n",
      "[11] F006-1,5x.ppm\n",
      "[66] F009-1,5x.ppm\n",
      "[68] F016,1,5x.ppm\n",
      "[10] F022-1,5x.ppm\n",
      "[33] F027-1,5x.ppm\n",
      "[30] F033-1,5x.ppm\n",
      "[39] F037-1,5x.ppm\n",
      "[67] F041-1,5x.ppm\n",
      "[77] F050-1,5x.ppm\n",
      "[96] F054-1,5x.ppm\n",
      "[41] F063-1,5x.ppm\n",
      "[23] F072-G-1,5x.ppm\n",
      "[83] F077-G-1,5x.ppm\n",
      "[18] F090-G-1,5x.ppm\n",
      "[44] F093-G-1,5x.ppm\n",
      "[15] F094-G-1,5x.ppm\n",
      "[63] F095-G-1,5x.ppm\n",
      "[99] F106-G-1,5x.ppm\n",
      "[53] F108-G-1,5x.ppm\n",
      "[47] F132-N-1,5x.ppm\n",
      "[254] S093-1,5x.ppm\n",
      "[216] S094-1,5x.ppm\n",
      "[288] S095-1,5x.ppm\n",
      "[213] S096-1,5x.ppm\n",
      "[279] S098-1,5x.ppm\n",
      "[207] S099-1,5x.ppm\n",
      "[209] S100-1,5x.ppm\n",
      "[259] S101-1,5x.ppm\n",
      "[300] s102-1,5x.ppm\n",
      "[268] S103-1,5x.ppm\n",
      "[278] S104-1,5x.ppm\n",
      "[251] S105-1,5x.ppm\n",
      "[267] S106-1,5x.ppm\n",
      "[281] S108-1,5x.ppm\n",
      "[244] S109-1,5x.ppm\n",
      "[229] S111-1,5x.ppm\n",
      "[247] S112-1,5x.ppm\n",
      "[208] S116-1,5x.ppm\n",
      "[261] S118-1,5x.ppm\n",
      "[234] S123-1,5x.ppm\n",
      "[131] B001-1,5x.ppm\n",
      "[193] B003,1,5x.ppm\n",
      "[140] B009-1,5x.ppm\n",
      "[167] B012-1,5X.ppm\n",
      "[195] B018-1,5x.ppm\n",
      "[162] B019-1,5x.ppm\n",
      "[158] B024-1,5x.ppm\n",
      "[110] B032-1,5x.ppm\n",
      "[137] B038-1,5x.ppm\n",
      "[134] B051-1,5x.ppm\n",
      "[129] B055-1,5x.ppm\n",
      "[194] B057-1,5x.ppm\n",
      "[153] B060-1,5x.ppm\n",
      "[150] B070-G-1,5x.ppm\n",
      "[174] B084-G-1,5x.ppm\n",
      "[100] B089-G-1,5x.ppm\n",
      "[127] B091-G-1,5x.ppm\n",
      "[106] B092-G-1,5x.ppm\n",
      "[149] B104-G-1,5x.ppm\n",
      "[142] B106-G-1,5x.ppm\n",
      "[175] B108-G-1,5x.ppm\n",
      "[51] F079-G-1,5x.ppm\n",
      "[98] F015-1,5x.ppm\n",
      "[14] F020-1,5x.ppm\n",
      "[54] F003-1,5x.ppm\n",
      "[61] F112-G-1,5x.ppm\n",
      "[50] F046-1,5x.ppm\n",
      "[86] F068-1,5x.ppm\n",
      "[80] F058-1,5x.ppm\n",
      "[48] F104-G-1,5x.ppm\n",
      "[72] F099-G-1,5x.ppm\n",
      "[6] F098-G-1,5x.ppm\n",
      "[9] F010-1,5x.ppm\n",
      "[13] F110-G-1,5x.ppm\n",
      "[79] F044-1,5x.ppm\n",
      "[65] F101-G-1,5x.ppm\n",
      "[75] F105-G-1,5x.ppm\n",
      "[70] F023-1,5x.ppm\n",
      "[38] F071-G-1,5x.ppm\n",
      "[24] F007-1,5x.ppm\n",
      "[4] F021-1,5x.ppm\n",
      "[266] S025-1,5x.ppm\n",
      "[236] S026-1,5x.ppm\n",
      "[218] S027-1,5x.ppm\n",
      "[299] S028-1,5x.ppm\n",
      "[243] S029-1,5x.ppm\n",
      "[289] S030-1,5x.ppm\n",
      "[284] S031-1,5x.ppm\n",
      "[286] S032-1,5x.ppm\n",
      "[201] S033-1,5x.ppm\n",
      "[221] S034-1,5x.ppm\n",
      "[253] S035-1,5x.ppm\n",
      "[214] S036-1,5x.ppm\n",
      "[276] S037-1,5x.ppm\n",
      "[204] S038-1,5x.ppm\n",
      "[256] S039-1,5x.ppm\n",
      "[230] S040-1,5x.ppm\n",
      "[202] S041-1,5x.ppm\n",
      "[219] S042-1,5x.ppm\n",
      "[206] S043-1,5x.ppm\n",
      "[298] S044-1,5x.ppm\n",
      "[104] B014-1,5x.ppm\n",
      "[125] B068-G-1,5x.ppm\n",
      "[196] B072-G-1,5x.ppm\n",
      "[188] B099-G-1,5x.ppm\n",
      "[103] B107-G-1,5x.ppm\n",
      "[151] B103-G-1,5x.ppm\n",
      "[108] B081-G-1,5x.ppm\n",
      "[172] B078-G-1,5x.ppm\n",
      "[144] B048-1,5x.ppm\n",
      "[133] B085-G-1,5x.ppm\n",
      "[148] B090-G-1,5x.ppm\n",
      "[191] B098-G-1,5x.ppm\n",
      "[157] B102-G-1,5x.ppm\n",
      "[169] B043-1,5x.ppm\n",
      "[168] B065-G-1,5x.ppm\n",
      "[163] B045-1,5x.ppm\n",
      "[177] B105_G-1,5x.ppm\n",
      "[198] B029-1,5x.ppm\n",
      "[109] B054-1,5x.ppm\n",
      "[113] B094-G-1,5x.ppm\n",
      "[69] F086-G-1,5x.ppm\n",
      "[85] F057-1,5x.ppm\n",
      "[46] F059-1,5x.ppm\n",
      "[12] F055-1,5x.ppm\n",
      "[84] F085-G-1,5x.ppm\n",
      "[3] F062-1,5x.ppm\n",
      "[22] F103-G-1,5x.ppm\n",
      "[25] F043-1,5x.ppm\n",
      "[89] F029-1,5x.ppm\n",
      "[34] F087-G-1,5x.ppm\n",
      "[49] F005-1,5x.ppm\n",
      "[0] F035-1,5x.ppm\n",
      "[92] F017-1,5x.ppm\n",
      "[45] F036-1,5x.ppm\n",
      "[56] F030-1,5x.ppm\n",
      "[29] F089-G-1,5x.ppm\n",
      "[16] F011-1,5x.ppm\n",
      "[36] F053-1,5x.ppm\n",
      "[74] F045-1,5x.ppm\n",
      "[93] F082-G-1,5x.ppm\n",
      "[225] S045-1,5x.ppm\n",
      "[264] S046-1,5x.ppm\n",
      "[255] S048-1,5x.ppm\n",
      "[292] S049-1,5x.ppm\n",
      "[293] S050-1,5x.ppm\n",
      "[280] S051-1,5x.ppm\n",
      "[297] S052-1,5x.ppm\n",
      "[239] S053-1,5x.ppm\n",
      "[241] S054-1,5x.ppm\n",
      "[258] S055-1,5.ppm\n",
      "[212] S056-1,5x.ppm\n",
      "[296] S057-1,5x.ppm\n",
      "[249] S058-1,5x.ppm\n",
      "[210] S059-1,5x.ppm\n",
      "[263] S060-1,5x.ppm\n",
      "[260] S061-1,5x.ppm\n",
      "[248] S062-1,5x.ppm\n",
      "[246] S063-1,5x.ppm\n",
      "[227] S064-1,5x.ppm\n",
      "[287] S065-1,5x.ppm\n",
      "[119] B097-G-1,5x.ppm\n",
      "[164] B017-1,5x.ppm\n",
      "[124] B095-G-1,5x.ppm\n",
      "[122] B080-G-1,5x.ppm\n",
      "[197] B053-1,5x.ppm\n",
      "[132] B100-G-1,5x.ppm\n",
      "[121] B066-G-1,5x.ppm\n",
      "[102] B069-G-1,5x.ppm\n",
      "[136] B034-1,5x.ppm\n",
      "[199] B007-1,5x.ppm\n",
      "[165] B031-1,5x.ppm\n",
      "[187] B073-G-1,5x.ppm\n",
      "[143] B028-1,5x.ppm\n",
      "[154] B039-1,5x.ppm\n",
      "[105] B058-1,5x.ppm\n",
      "[147] B016-1,5x.ppm\n",
      "[186] B030-1,5x.ppm\n",
      "[156] B076-G-1,5x.ppm\n",
      "[183] B025-1,5x.ppm\n",
      "[120] B047-1,5x.ppm\n",
      "[59] F066-1,5x.ppm\n",
      "[7] F115-G-1,5x.ppm\n",
      "[73] F051-1,5x.ppm\n",
      "[64] F047-1,5x.ppm\n",
      "[57] F076-G-1,5x.ppm\n",
      "[82] F114-G-1,5x.ppm\n",
      "[78] F092-G-1,5x.ppm\n",
      "[40] F088-G-1,5x.ppm\n",
      "[90] F012-1,5x.ppm\n",
      "[27] F083-G-1,5x.ppm\n",
      "[31] F078-G-1,5x.ppm\n",
      "[5] F042-1,5x.ppm\n",
      "[52] F100-G-1,5x.ppm\n",
      "[32] F014-1,5x.ppm\n",
      "[28] F102-G-1,5x.ppm\n",
      "[26] F031-1,5x.ppm\n",
      "[37] F040-1,5x.ppm\n",
      "[60] F111-G-1,5x.ppm\n",
      "[21] F052-1,5x.ppm\n",
      "[58] F109-G-1,5x.ppm\n",
      "[238] S067-1,5x.ppm\n",
      "[272] S068-1,5x.ppm\n",
      "[217] S069-1,5x.ppm\n",
      "[223] S071-1,5x.ppm\n",
      "[277] S072-1,5x.ppm\n",
      "[235] S073-1,5x.ppm\n",
      "[222] S074-1,5x.ppm\n",
      "[215] S075-1,5x.ppm\n",
      "[237] S076-1,5x.ppm\n",
      "[233] S078-1,5x.ppm\n",
      "[270] S079-1,5x.ppm\n",
      "[265] S080-1,5x.ppm\n",
      "[252] S081-1,5x.ppm\n",
      "[220] S082-1,5x.ppm\n",
      "[273] S083-1,5x.ppm\n",
      "[275] S087-1,5x.ppm\n",
      "[231] S088-1,5x.ppm\n",
      "[295] S090-1,5x.ppm\n",
      "[283] S091-1,5x.ppm\n",
      "[269] S092-1,5x.ppm\n",
      "[190] B013-1,5x.ppm\n",
      "[123] B006-1,5x.ppm\n",
      "[178] B041-1,5x.ppm\n",
      "[117] B059-M-1,5x.ppm\n",
      "[179] B004-1,5x.ppm\n",
      "[116] B050-1,5x.ppm\n",
      "[200] B088-G01,5x.ppm\n",
      "[128] B026-15x.ppm\n",
      "[185] B052-1,5x.ppm\n",
      "[159] B008-1,5x.ppm\n",
      "[184] B010-1,5x.ppm\n",
      "[111] B065-1,5x.ppm\n",
      "[170] B067-G-1,5x.ppm\n",
      "[118] B087-G-1,5x.ppm\n",
      "[171] B040-1,5x.ppm\n",
      "[107] B079-G-1,5x.ppm\n",
      "[192] B044-1,5x.ppm\n",
      "[114] B005-1,5x.ppm\n",
      "[161] B063-G-1,5x.ppm\n",
      "[181] B046-1,5x.ppm\n",
      "[0] Train fold size: 241\n",
      "[11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181]\n",
      "Datasets/Folds/Fold-Test1.txt\n",
      "[1] F025-1,5x.ppm\n",
      "[35] F107-G-1,5x.ppm\n",
      "[95] F013-1,5x.ppm\n",
      "[91] F034-1,5x.ppm\n",
      "[62] F113-G-1,5x.ppm\n",
      "[2] F084-G-1,5x.ppm\n",
      "[76] F049-1,5x.ppm\n",
      "[81] F060-1,5x.ppm\n",
      "[43] F075-G-1,5x.ppm\n",
      "[71] F097-G-1,5x.ppm\n",
      "[17] F004-1,5x.ppm\n",
      "[55] F039-1,5x.ppm\n",
      "[88] F061-1,5x.ppm\n",
      "[42] F080-G-1,5x.ppm\n",
      "[19] F026-1,5x.ppm\n",
      "[87] F064-1,5x.ppm\n",
      "[20] F028-1,5x.ppm\n",
      "[97] F056-1,5x.ppm\n",
      "[94] F091-G-1,5x.ppm\n",
      "[8] F048-1,5x.ppm\n",
      "[262] S001-1,5x.ppm\n",
      "[245] S003-1,5x.ppm\n",
      "[257] S004-1,5x.ppm\n",
      "[250] S005-1,5x.ppm\n",
      "[291] S006-1,5x.ppm\n",
      "[285] S007-1,5x.ppm\n",
      "[290] S008-1,5x.ppm\n",
      "[274] S009-1,5x.ppm\n",
      "[226] S010-1,5x.ppm\n",
      "[205] S011-1,5x.ppm\n",
      "[232] S012-1,5x.ppm\n",
      "[228] S015-1,5x.ppm\n",
      "[203] S016-1,5x.ppm\n",
      "[240] S018-1,5x.ppm\n",
      "[242] S019-1,5x.ppm\n",
      "[282] S020-1,5x.ppm\n",
      "[211] S021-1,5x.ppm\n",
      "[271] S022-1,5x.ppm\n",
      "[294] S023-1,5x.ppm\n",
      "[224] S024-1,5x.ppm\n",
      "[152] B077-G-1,5x.ppm\n",
      "[176] B074-G-1,5x.ppm\n",
      "[160] B056-1,5x.ppm\n",
      "[155] B061-M-1,5x.ppm\n",
      "[189] B020-1,5x.ppm\n",
      "[180] B015-1,5x.ppm\n",
      "[146] B101-G-1,5x.ppm\n",
      "[126] B082-G-1,5x.ppm\n",
      "[115] B093-G-1,5x.ppm\n",
      "[182] B042-1,5x.ppm\n",
      "[166] B062-G-1,5x.ppm\n",
      "[141] B002-1,5x.ppm\n",
      "[139] B033-15,x.ppm\n",
      "[130] B049-1,5x.ppm\n",
      "[145] B083-G-1,5x.ppm\n",
      "[138] B075-G-1,5x.ppm\n",
      "[112] B096-G-1,5x.ppm\n",
      "[135] B086-G-1,5x.ppm\n",
      "[173] B071-G-1,5x.ppm\n",
      "[101] B027-1,5x.ppm\n",
      "[0] Test fold size: 60\n",
      "[1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101]\n",
      "Datasets/Folds/Fold-Train2.txt\n",
      "[1] F025-1,5x.ppm\n",
      "[35] F107-G-1,5x.ppm\n",
      "[95] F013-1,5x.ppm\n",
      "[91] F034-1,5x.ppm\n",
      "[62] F113-G-1,5x.ppm\n",
      "[2] F084-G-1,5x.ppm\n",
      "[76] F049-1,5x.ppm\n",
      "[81] F060-1,5x.ppm\n",
      "[43] F075-G-1,5x.ppm\n",
      "[71] F097-G-1,5x.ppm\n",
      "[17] F004-1,5x.ppm\n",
      "[55] F039-1,5x.ppm\n",
      "[88] F061-1,5x.ppm\n",
      "[42] F080-G-1,5x.ppm\n",
      "[19] F026-1,5x.ppm\n",
      "[87] F064-1,5x.ppm\n",
      "[20] F028-1,5x.ppm\n",
      "[97] F056-1,5x.ppm\n",
      "[94] F091-G-1,5x.ppm\n",
      "[8] F048-1,5x.ppm\n",
      "[262] S001-1,5x.ppm\n",
      "[245] S003-1,5x.ppm\n",
      "[257] S004-1,5x.ppm\n",
      "[250] S005-1,5x.ppm\n",
      "[291] S006-1,5x.ppm\n",
      "[285] S007-1,5x.ppm\n",
      "[290] S008-1,5x.ppm\n",
      "[274] S009-1,5x.ppm\n",
      "[226] S010-1,5x.ppm\n",
      "[205] S011-1,5x.ppm\n",
      "[232] S012-1,5x.ppm\n",
      "[228] S015-1,5x.ppm\n",
      "[203] S016-1,5x.ppm\n",
      "[240] S018-1,5x.ppm\n",
      "[242] S019-1,5x.ppm\n",
      "[282] S020-1,5x.ppm\n",
      "[211] S021-1,5x.ppm\n",
      "[271] S022-1,5x.ppm\n",
      "[294] S023-1,5x.ppm\n",
      "[224] S024-1,5x.ppm\n",
      "[152] B077-G-1,5x.ppm\n",
      "[176] B074-G-1,5x.ppm\n",
      "[160] B056-1,5x.ppm\n",
      "[155] B061-M-1,5x.ppm\n",
      "[189] B020-1,5x.ppm\n",
      "[180] B015-1,5x.ppm\n",
      "[146] B101-G-1,5x.ppm\n",
      "[126] B082-G-1,5x.ppm\n",
      "[115] B093-G-1,5x.ppm\n",
      "[182] B042-1,5x.ppm\n",
      "[166] B062-G-1,5x.ppm\n",
      "[141] B002-1,5x.ppm\n",
      "[139] B033-15,x.ppm\n",
      "[130] B049-1,5x.ppm\n",
      "[145] B083-G-1,5x.ppm\n",
      "[138] B075-G-1,5x.ppm\n",
      "[112] B096-G-1,5x.ppm\n",
      "[135] B086-G-1,5x.ppm\n",
      "[173] B071-G-1,5x.ppm\n",
      "[101] B027-1,5x.ppm\n",
      "[51] F079-G-1,5x.ppm\n",
      "[98] F015-1,5x.ppm\n",
      "[14] F020-1,5x.ppm\n",
      "[54] F003-1,5x.ppm\n",
      "[61] F112-G-1,5x.ppm\n",
      "[50] F046-1,5x.ppm\n",
      "[86] F068-1,5x.ppm\n",
      "[80] F058-1,5x.ppm\n",
      "[48] F104-G-1,5x.ppm\n",
      "[72] F099-G-1,5x.ppm\n",
      "[6] F098-G-1,5x.ppm\n",
      "[9] F010-1,5x.ppm\n",
      "[13] F110-G-1,5x.ppm\n",
      "[79] F044-1,5x.ppm\n",
      "[65] F101-G-1,5x.ppm\n",
      "[75] F105-G-1,5x.ppm\n",
      "[70] F023-1,5x.ppm\n",
      "[38] F071-G-1,5x.ppm\n",
      "[24] F007-1,5x.ppm\n",
      "[4] F021-1,5x.ppm\n",
      "[266] S025-1,5x.ppm\n",
      "[236] S026-1,5x.ppm\n",
      "[218] S027-1,5x.ppm\n",
      "[299] S028-1,5x.ppm\n",
      "[243] S029-1,5x.ppm\n",
      "[289] S030-1,5x.ppm\n",
      "[284] S031-1,5x.ppm\n",
      "[286] S032-1,5x.ppm\n",
      "[201] S033-1,5x.ppm\n",
      "[221] S034-1,5x.ppm\n",
      "[253] S035-1,5x.ppm\n",
      "[214] S036-1,5x.ppm\n",
      "[276] S037-1,5x.ppm\n",
      "[204] S038-1,5x.ppm\n",
      "[256] S039-1,5x.ppm\n",
      "[230] S040-1,5x.ppm\n",
      "[202] S041-1,5x.ppm\n",
      "[219] S042-1,5x.ppm\n",
      "[206] S043-1,5x.ppm\n",
      "[298] S044-1,5x.ppm\n",
      "[104] B014-1,5x.ppm\n",
      "[125] B068-G-1,5x.ppm\n",
      "[196] B072-G-1,5x.ppm\n",
      "[188] B099-G-1,5x.ppm\n",
      "[103] B107-G-1,5x.ppm\n",
      "[151] B103-G-1,5x.ppm\n",
      "[108] B081-G-1,5x.ppm\n",
      "[172] B078-G-1,5x.ppm\n",
      "[144] B048-1,5x.ppm\n",
      "[133] B085-G-1,5x.ppm\n",
      "[148] B090-G-1,5x.ppm\n",
      "[191] B098-G-1,5x.ppm\n",
      "[157] B102-G-1,5x.ppm\n",
      "[169] B043-1,5x.ppm\n",
      "[168] B065-G-1,5x.ppm\n",
      "[163] B045-1,5x.ppm\n",
      "[177] B105_G-1,5x.ppm\n",
      "[198] B029-1,5x.ppm\n",
      "[109] B054-1,5x.ppm\n",
      "[113] B094-G-1,5x.ppm\n",
      "[69] F086-G-1,5x.ppm\n",
      "[85] F057-1,5x.ppm\n",
      "[46] F059-1,5x.ppm\n",
      "[12] F055-1,5x.ppm\n",
      "[84] F085-G-1,5x.ppm\n",
      "[3] F062-1,5x.ppm\n",
      "[22] F103-G-1,5x.ppm\n",
      "[25] F043-1,5x.ppm\n",
      "[89] F029-1,5x.ppm\n",
      "[34] F087-G-1,5x.ppm\n",
      "[49] F005-1,5x.ppm\n",
      "[0] F035-1,5x.ppm\n",
      "[92] F017-1,5x.ppm\n",
      "[45] F036-1,5x.ppm\n",
      "[56] F030-1,5x.ppm\n",
      "[29] F089-G-1,5x.ppm\n",
      "[16] F011-1,5x.ppm\n",
      "[36] F053-1,5x.ppm\n",
      "[74] F045-1,5x.ppm\n",
      "[93] F082-G-1,5x.ppm\n",
      "[225] S045-1,5x.ppm\n",
      "[264] S046-1,5x.ppm\n",
      "[255] S048-1,5x.ppm\n",
      "[292] S049-1,5x.ppm\n",
      "[293] S050-1,5x.ppm\n",
      "[280] S051-1,5x.ppm\n",
      "[297] S052-1,5x.ppm\n",
      "[239] S053-1,5x.ppm\n",
      "[241] S054-1,5x.ppm\n",
      "[258] S055-1,5.ppm\n",
      "[212] S056-1,5x.ppm\n",
      "[296] S057-1,5x.ppm\n",
      "[249] S058-1,5x.ppm\n",
      "[210] S059-1,5x.ppm\n",
      "[263] S060-1,5x.ppm\n",
      "[260] S061-1,5x.ppm\n",
      "[248] S062-1,5x.ppm\n",
      "[246] S063-1,5x.ppm\n",
      "[227] S064-1,5x.ppm\n",
      "[287] S065-1,5x.ppm\n",
      "[119] B097-G-1,5x.ppm\n",
      "[164] B017-1,5x.ppm\n",
      "[124] B095-G-1,5x.ppm\n",
      "[122] B080-G-1,5x.ppm\n",
      "[197] B053-1,5x.ppm\n",
      "[132] B100-G-1,5x.ppm\n",
      "[121] B066-G-1,5x.ppm\n",
      "[102] B069-G-1,5x.ppm\n",
      "[136] B034-1,5x.ppm\n",
      "[199] B007-1,5x.ppm\n",
      "[165] B031-1,5x.ppm\n",
      "[187] B073-G-1,5x.ppm\n",
      "[143] B028-1,5x.ppm\n",
      "[154] B039-1,5x.ppm\n",
      "[105] B058-1,5x.ppm\n",
      "[147] B016-1,5x.ppm\n",
      "[186] B030-1,5x.ppm\n",
      "[156] B076-G-1,5x.ppm\n",
      "[183] B025-1,5x.ppm\n",
      "[120] B047-1,5x.ppm\n",
      "[59] F066-1,5x.ppm\n",
      "[7] F115-G-1,5x.ppm\n",
      "[73] F051-1,5x.ppm\n",
      "[64] F047-1,5x.ppm\n",
      "[57] F076-G-1,5x.ppm\n",
      "[82] F114-G-1,5x.ppm\n",
      "[78] F092-G-1,5x.ppm\n",
      "[40] F088-G-1,5x.ppm\n",
      "[90] F012-1,5x.ppm\n",
      "[27] F083-G-1,5x.ppm\n",
      "[31] F078-G-1,5x.ppm\n",
      "[5] F042-1,5x.ppm\n",
      "[52] F100-G-1,5x.ppm\n",
      "[32] F014-1,5x.ppm\n",
      "[28] F102-G-1,5x.ppm\n",
      "[26] F031-1,5x.ppm\n",
      "[37] F040-1,5x.ppm\n",
      "[60] F111-G-1,5x.ppm\n",
      "[21] F052-1,5x.ppm\n",
      "[58] F109-G-1,5x.ppm\n",
      "[238] S067-1,5x.ppm\n",
      "[272] S068-1,5x.ppm\n",
      "[217] S069-1,5x.ppm\n",
      "[223] S071-1,5x.ppm\n",
      "[277] S072-1,5x.ppm\n",
      "[235] S073-1,5x.ppm\n",
      "[222] S074-1,5x.ppm\n",
      "[215] S075-1,5x.ppm\n",
      "[237] S076-1,5x.ppm\n",
      "[233] S078-1,5x.ppm\n",
      "[270] S079-1,5x.ppm\n",
      "[265] S080-1,5x.ppm\n",
      "[252] S081-1,5x.ppm\n",
      "[220] S082-1,5x.ppm\n",
      "[273] S083-1,5x.ppm\n",
      "[275] S087-1,5x.ppm\n",
      "[231] S088-1,5x.ppm\n",
      "[295] S090-1,5x.ppm\n",
      "[283] S091-1,5x.ppm\n",
      "[269] S092-1,5x.ppm\n",
      "[190] B013-1,5x.ppm\n",
      "[123] B006-1,5x.ppm\n",
      "[178] B041-1,5x.ppm\n",
      "[117] B059-M-1,5x.ppm\n",
      "[179] B004-1,5x.ppm\n",
      "[116] B050-1,5x.ppm\n",
      "[200] B088-G01,5x.ppm\n",
      "[128] B026-15x.ppm\n",
      "[185] B052-1,5x.ppm\n",
      "[159] B008-1,5x.ppm\n",
      "[184] B010-1,5x.ppm\n",
      "[111] B065-1,5x.ppm\n",
      "[170] B067-G-1,5x.ppm\n",
      "[118] B087-G-1,5x.ppm\n",
      "[171] B040-1,5x.ppm\n",
      "[107] B079-G-1,5x.ppm\n",
      "[192] B044-1,5x.ppm\n",
      "[114] B005-1,5x.ppm\n",
      "[161] B063-G-1,5x.ppm\n",
      "[181] B046-1,5x.ppm\n",
      "[1] Train fold size: 240\n",
      "[1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181]\n",
      "Datasets/Folds/Fold-Test2.txt\n",
      "[11] F006-1,5x.ppm\n",
      "[66] F009-1,5x.ppm\n",
      "[68] F016,1,5x.ppm\n",
      "[10] F022-1,5x.ppm\n",
      "[33] F027-1,5x.ppm\n",
      "[30] F033-1,5x.ppm\n",
      "[39] F037-1,5x.ppm\n",
      "[67] F041-1,5x.ppm\n",
      "[77] F050-1,5x.ppm\n",
      "[96] F054-1,5x.ppm\n",
      "[41] F063-1,5x.ppm\n",
      "[23] F072-G-1,5x.ppm\n",
      "[83] F077-G-1,5x.ppm\n",
      "[18] F090-G-1,5x.ppm\n",
      "[44] F093-G-1,5x.ppm\n",
      "[15] F094-G-1,5x.ppm\n",
      "[63] F095-G-1,5x.ppm\n",
      "[99] F106-G-1,5x.ppm\n",
      "[53] F108-G-1,5x.ppm\n",
      "[47] F132-N-1,5x.ppm\n",
      "[254] S093-1,5x.ppm\n",
      "[216] S094-1,5x.ppm\n",
      "[288] S095-1,5x.ppm\n",
      "[213] S096-1,5x.ppm\n",
      "[279] S098-1,5x.ppm\n",
      "[207] S099-1,5x.ppm\n",
      "[209] S100-1,5x.ppm\n",
      "[259] S101-1,5x.ppm\n",
      "[300] s102-1,5x.ppm\n",
      "[268] S103-1,5x.ppm\n",
      "[278] S104-1,5x.ppm\n",
      "[251] S105-1,5x.ppm\n",
      "[267] S106-1,5x.ppm\n",
      "[281] S108-1,5x.ppm\n",
      "[244] S109-1,5x.ppm\n",
      "[229] S111-1,5x.ppm\n",
      "[247] S112-1,5x.ppm\n",
      "[208] S116-1,5x.ppm\n",
      "[261] S118-1,5x.ppm\n",
      "[234] S123-1,5x.ppm\n",
      "[131] B001-1,5x.ppm\n",
      "[193] B003,1,5x.ppm\n",
      "[140] B009-1,5x.ppm\n",
      "[167] B012-1,5X.ppm\n",
      "[195] B018-1,5x.ppm\n",
      "[162] B019-1,5x.ppm\n",
      "[158] B024-1,5x.ppm\n",
      "[110] B032-1,5x.ppm\n",
      "[137] B038-1,5x.ppm\n",
      "[134] B051-1,5x.ppm\n",
      "[129] B055-1,5x.ppm\n",
      "[194] B057-1,5x.ppm\n",
      "[153] B060-1,5x.ppm\n",
      "[150] B070-G-1,5x.ppm\n",
      "[174] B084-G-1,5x.ppm\n",
      "[100] B089-G-1,5x.ppm\n",
      "[127] B091-G-1,5x.ppm\n",
      "[106] B092-G-1,5x.ppm\n",
      "[149] B104-G-1,5x.ppm\n",
      "[142] B106-G-1,5x.ppm\n",
      "[175] B108-G-1,5x.ppm\n",
      "[1] Test fold size: 61\n",
      "[11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175]\n",
      "Datasets/Folds/Fold-Train3.txt\n",
      "[1] F025-1,5x.ppm\n",
      "[35] F107-G-1,5x.ppm\n",
      "[95] F013-1,5x.ppm\n",
      "[91] F034-1,5x.ppm\n",
      "[62] F113-G-1,5x.ppm\n",
      "[2] F084-G-1,5x.ppm\n",
      "[76] F049-1,5x.ppm\n",
      "[81] F060-1,5x.ppm\n",
      "[43] F075-G-1,5x.ppm\n",
      "[71] F097-G-1,5x.ppm\n",
      "[17] F004-1,5x.ppm\n",
      "[55] F039-1,5x.ppm\n",
      "[88] F061-1,5x.ppm\n",
      "[42] F080-G-1,5x.ppm\n",
      "[19] F026-1,5x.ppm\n",
      "[87] F064-1,5x.ppm\n",
      "[20] F028-1,5x.ppm\n",
      "[97] F056-1,5x.ppm\n",
      "[94] F091-G-1,5x.ppm\n",
      "[8] F048-1,5x.ppm\n",
      "[262] S001-1,5x.ppm\n",
      "[245] S003-1,5x.ppm\n",
      "[257] S004-1,5x.ppm\n",
      "[250] S005-1,5x.ppm\n",
      "[291] S006-1,5x.ppm\n",
      "[285] S007-1,5x.ppm\n",
      "[290] S008-1,5x.ppm\n",
      "[274] S009-1,5x.ppm\n",
      "[226] S010-1,5x.ppm\n",
      "[205] S011-1,5x.ppm\n",
      "[232] S012-1,5x.ppm\n",
      "[228] S015-1,5x.ppm\n",
      "[203] S016-1,5x.ppm\n",
      "[240] S018-1,5x.ppm\n",
      "[242] S019-1,5x.ppm\n",
      "[282] S020-1,5x.ppm\n",
      "[211] S021-1,5x.ppm\n",
      "[271] S022-1,5x.ppm\n",
      "[294] S023-1,5x.ppm\n",
      "[224] S024-1,5x.ppm\n",
      "[152] B077-G-1,5x.ppm\n",
      "[176] B074-G-1,5x.ppm\n",
      "[160] B056-1,5x.ppm\n",
      "[155] B061-M-1,5x.ppm\n",
      "[189] B020-1,5x.ppm\n",
      "[180] B015-1,5x.ppm\n",
      "[146] B101-G-1,5x.ppm\n",
      "[126] B082-G-1,5x.ppm\n",
      "[115] B093-G-1,5x.ppm\n",
      "[182] B042-1,5x.ppm\n",
      "[166] B062-G-1,5x.ppm\n",
      "[141] B002-1,5x.ppm\n",
      "[139] B033-15,x.ppm\n",
      "[130] B049-1,5x.ppm\n",
      "[145] B083-G-1,5x.ppm\n",
      "[138] B075-G-1,5x.ppm\n",
      "[112] B096-G-1,5x.ppm\n",
      "[135] B086-G-1,5x.ppm\n",
      "[173] B071-G-1,5x.ppm\n",
      "[101] B027-1,5x.ppm\n",
      "[11] F006-1,5x.ppm\n",
      "[66] F009-1,5x.ppm\n",
      "[68] F016,1,5x.ppm\n",
      "[10] F022-1,5x.ppm\n",
      "[33] F027-1,5x.ppm\n",
      "[30] F033-1,5x.ppm\n",
      "[39] F037-1,5x.ppm\n",
      "[67] F041-1,5x.ppm\n",
      "[77] F050-1,5x.ppm\n",
      "[96] F054-1,5x.ppm\n",
      "[41] F063-1,5x.ppm\n",
      "[23] F072-G-1,5x.ppm\n",
      "[83] F077-G-1,5x.ppm\n",
      "[18] F090-G-1,5x.ppm\n",
      "[44] F093-G-1,5x.ppm\n",
      "[15] F094-G-1,5x.ppm\n",
      "[63] F095-G-1,5x.ppm\n",
      "[99] F106-G-1,5x.ppm\n",
      "[53] F108-G-1,5x.ppm\n",
      "[47] F132-N-1,5x.ppm\n",
      "[254] S093-1,5x.ppm\n",
      "[216] S094-1,5x.ppm\n",
      "[288] S095-1,5x.ppm\n",
      "[213] S096-1,5x.ppm\n",
      "[279] S098-1,5x.ppm\n",
      "[207] S099-1,5x.ppm\n",
      "[209] S100-1,5x.ppm\n",
      "[259] S101-1,5x.ppm\n",
      "[300] s102-1,5x.ppm\n",
      "[268] S103-1,5x.ppm\n",
      "[278] S104-1,5x.ppm\n",
      "[251] S105-1,5x.ppm\n",
      "[267] S106-1,5x.ppm\n",
      "[281] S108-1,5x.ppm\n",
      "[244] S109-1,5x.ppm\n",
      "[229] S111-1,5x.ppm\n",
      "[247] S112-1,5x.ppm\n",
      "[208] S116-1,5x.ppm\n",
      "[261] S118-1,5x.ppm\n",
      "[234] S123-1,5x.ppm\n",
      "[131] B001-1,5x.ppm\n",
      "[193] B003,1,5x.ppm\n",
      "[140] B009-1,5x.ppm\n",
      "[167] B012-1,5X.ppm\n",
      "[195] B018-1,5x.ppm\n",
      "[162] B019-1,5x.ppm\n",
      "[158] B024-1,5x.ppm\n",
      "[110] B032-1,5x.ppm\n",
      "[137] B038-1,5x.ppm\n",
      "[134] B051-1,5x.ppm\n",
      "[129] B055-1,5x.ppm\n",
      "[194] B057-1,5x.ppm\n",
      "[153] B060-1,5x.ppm\n",
      "[150] B070-G-1,5x.ppm\n",
      "[174] B084-G-1,5x.ppm\n",
      "[100] B089-G-1,5x.ppm\n",
      "[127] B091-G-1,5x.ppm\n",
      "[106] B092-G-1,5x.ppm\n",
      "[149] B104-G-1,5x.ppm\n",
      "[142] B106-G-1,5x.ppm\n",
      "[175] B108-G-1,5x.ppm\n",
      "[69] F086-G-1,5x.ppm\n",
      "[85] F057-1,5x.ppm\n",
      "[46] F059-1,5x.ppm\n",
      "[12] F055-1,5x.ppm\n",
      "[84] F085-G-1,5x.ppm\n",
      "[3] F062-1,5x.ppm\n",
      "[22] F103-G-1,5x.ppm\n",
      "[25] F043-1,5x.ppm\n",
      "[89] F029-1,5x.ppm\n",
      "[34] F087-G-1,5x.ppm\n",
      "[49] F005-1,5x.ppm\n",
      "[0] F035-1,5x.ppm\n",
      "[92] F017-1,5x.ppm\n",
      "[45] F036-1,5x.ppm\n",
      "[56] F030-1,5x.ppm\n",
      "[29] F089-G-1,5x.ppm\n",
      "[16] F011-1,5x.ppm\n",
      "[36] F053-1,5x.ppm\n",
      "[74] F045-1,5x.ppm\n",
      "[93] F082-G-1,5x.ppm\n",
      "[225] S045-1,5x.ppm\n",
      "[264] S046-1,5x.ppm\n",
      "[255] S048-1,5x.ppm\n",
      "[292] S049-1,5x.ppm\n",
      "[293] S050-1,5x.ppm\n",
      "[280] S051-1,5x.ppm\n",
      "[297] S052-1,5x.ppm\n",
      "[239] S053-1,5x.ppm\n",
      "[241] S054-1,5x.ppm\n",
      "[258] S055-1,5.ppm\n",
      "[212] S056-1,5x.ppm\n",
      "[296] S057-1,5x.ppm\n",
      "[249] S058-1,5x.ppm\n",
      "[210] S059-1,5x.ppm\n",
      "[263] S060-1,5x.ppm\n",
      "[260] S061-1,5x.ppm\n",
      "[248] S062-1,5x.ppm\n",
      "[246] S063-1,5x.ppm\n",
      "[227] S064-1,5x.ppm\n",
      "[287] S065-1,5x.ppm\n",
      "[119] B097-G-1,5x.ppm\n",
      "[164] B017-1,5x.ppm\n",
      "[124] B095-G-1,5x.ppm\n",
      "[122] B080-G-1,5x.ppm\n",
      "[197] B053-1,5x.ppm\n",
      "[132] B100-G-1,5x.ppm\n",
      "[121] B066-G-1,5x.ppm\n",
      "[102] B069-G-1,5x.ppm\n",
      "[136] B034-1,5x.ppm\n",
      "[199] B007-1,5x.ppm\n",
      "[165] B031-1,5x.ppm\n",
      "[187] B073-G-1,5x.ppm\n",
      "[143] B028-1,5x.ppm\n",
      "[154] B039-1,5x.ppm\n",
      "[105] B058-1,5x.ppm\n",
      "[147] B016-1,5x.ppm\n",
      "[186] B030-1,5x.ppm\n",
      "[156] B076-G-1,5x.ppm\n",
      "[183] B025-1,5x.ppm\n",
      "[120] B047-1,5x.ppm\n",
      "[59] F066-1,5x.ppm\n",
      "[7] F115-G-1,5x.ppm\n",
      "[73] F051-1,5x.ppm\n",
      "[64] F047-1,5x.ppm\n",
      "[57] F076-G-1,5x.ppm\n",
      "[82] F114-G-1,5x.ppm\n",
      "[78] F092-G-1,5x.ppm\n",
      "[40] F088-G-1,5x.ppm\n",
      "[90] F012-1,5x.ppm\n",
      "[27] F083-G-1,5x.ppm\n",
      "[31] F078-G-1,5x.ppm\n",
      "[5] F042-1,5x.ppm\n",
      "[52] F100-G-1,5x.ppm\n",
      "[32] F014-1,5x.ppm\n",
      "[28] F102-G-1,5x.ppm\n",
      "[26] F031-1,5x.ppm\n",
      "[37] F040-1,5x.ppm\n",
      "[60] F111-G-1,5x.ppm\n",
      "[21] F052-1,5x.ppm\n",
      "[58] F109-G-1,5x.ppm\n",
      "[238] S067-1,5x.ppm\n",
      "[272] S068-1,5x.ppm\n",
      "[217] S069-1,5x.ppm\n",
      "[223] S071-1,5x.ppm\n",
      "[277] S072-1,5x.ppm\n",
      "[235] S073-1,5x.ppm\n",
      "[222] S074-1,5x.ppm\n",
      "[215] S075-1,5x.ppm\n",
      "[237] S076-1,5x.ppm\n",
      "[233] S078-1,5x.ppm\n",
      "[270] S079-1,5x.ppm\n",
      "[265] S080-1,5x.ppm\n",
      "[252] S081-1,5x.ppm\n",
      "[220] S082-1,5x.ppm\n",
      "[273] S083-1,5x.ppm\n",
      "[275] S087-1,5x.ppm\n",
      "[231] S088-1,5x.ppm\n",
      "[295] S090-1,5x.ppm\n",
      "[283] S091-1,5x.ppm\n",
      "[269] S092-1,5x.ppm\n",
      "[190] B013-1,5x.ppm\n",
      "[123] B006-1,5x.ppm\n",
      "[178] B041-1,5x.ppm\n",
      "[117] B059-M-1,5x.ppm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179] B004-1,5x.ppm\n",
      "[116] B050-1,5x.ppm\n",
      "[200] B088-G01,5x.ppm\n",
      "[128] B026-15x.ppm\n",
      "[185] B052-1,5x.ppm\n",
      "[159] B008-1,5x.ppm\n",
      "[184] B010-1,5x.ppm\n",
      "[111] B065-1,5x.ppm\n",
      "[170] B067-G-1,5x.ppm\n",
      "[118] B087-G-1,5x.ppm\n",
      "[171] B040-1,5x.ppm\n",
      "[107] B079-G-1,5x.ppm\n",
      "[192] B044-1,5x.ppm\n",
      "[114] B005-1,5x.ppm\n",
      "[161] B063-G-1,5x.ppm\n",
      "[181] B046-1,5x.ppm\n",
      "[2] Train fold size: 241\n",
      "[1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181]\n",
      "Datasets/Folds/Fold-Test3.txt\n",
      "[51] F079-G-1,5x.ppm\n",
      "[98] F015-1,5x.ppm\n",
      "[14] F020-1,5x.ppm\n",
      "[54] F003-1,5x.ppm\n",
      "[61] F112-G-1,5x.ppm\n",
      "[50] F046-1,5x.ppm\n",
      "[86] F068-1,5x.ppm\n",
      "[80] F058-1,5x.ppm\n",
      "[48] F104-G-1,5x.ppm\n",
      "[72] F099-G-1,5x.ppm\n",
      "[6] F098-G-1,5x.ppm\n",
      "[9] F010-1,5x.ppm\n",
      "[13] F110-G-1,5x.ppm\n",
      "[79] F044-1,5x.ppm\n",
      "[65] F101-G-1,5x.ppm\n",
      "[75] F105-G-1,5x.ppm\n",
      "[70] F023-1,5x.ppm\n",
      "[38] F071-G-1,5x.ppm\n",
      "[24] F007-1,5x.ppm\n",
      "[4] F021-1,5x.ppm\n",
      "[266] S025-1,5x.ppm\n",
      "[236] S026-1,5x.ppm\n",
      "[218] S027-1,5x.ppm\n",
      "[299] S028-1,5x.ppm\n",
      "[243] S029-1,5x.ppm\n",
      "[289] S030-1,5x.ppm\n",
      "[284] S031-1,5x.ppm\n",
      "[286] S032-1,5x.ppm\n",
      "[201] S033-1,5x.ppm\n",
      "[221] S034-1,5x.ppm\n",
      "[253] S035-1,5x.ppm\n",
      "[214] S036-1,5x.ppm\n",
      "[276] S037-1,5x.ppm\n",
      "[204] S038-1,5x.ppm\n",
      "[256] S039-1,5x.ppm\n",
      "[230] S040-1,5x.ppm\n",
      "[202] S041-1,5x.ppm\n",
      "[219] S042-1,5x.ppm\n",
      "[206] S043-1,5x.ppm\n",
      "[298] S044-1,5x.ppm\n",
      "[104] B014-1,5x.ppm\n",
      "[125] B068-G-1,5x.ppm\n",
      "[196] B072-G-1,5x.ppm\n",
      "[188] B099-G-1,5x.ppm\n",
      "[103] B107-G-1,5x.ppm\n",
      "[151] B103-G-1,5x.ppm\n",
      "[108] B081-G-1,5x.ppm\n",
      "[172] B078-G-1,5x.ppm\n",
      "[144] B048-1,5x.ppm\n",
      "[133] B085-G-1,5x.ppm\n",
      "[148] B090-G-1,5x.ppm\n",
      "[191] B098-G-1,5x.ppm\n",
      "[157] B102-G-1,5x.ppm\n",
      "[169] B043-1,5x.ppm\n",
      "[168] B065-G-1,5x.ppm\n",
      "[163] B045-1,5x.ppm\n",
      "[177] B105_G-1,5x.ppm\n",
      "[198] B029-1,5x.ppm\n",
      "[109] B054-1,5x.ppm\n",
      "[113] B094-G-1,5x.ppm\n",
      "[2] Test fold size: 60\n",
      "[51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113]\n",
      "Datasets/Folds/Fold-Train4.txt\n",
      "[1] F025-1,5x.ppm\n",
      "[35] F107-G-1,5x.ppm\n",
      "[95] F013-1,5x.ppm\n",
      "[91] F034-1,5x.ppm\n",
      "[62] F113-G-1,5x.ppm\n",
      "[2] F084-G-1,5x.ppm\n",
      "[76] F049-1,5x.ppm\n",
      "[81] F060-1,5x.ppm\n",
      "[43] F075-G-1,5x.ppm\n",
      "[71] F097-G-1,5x.ppm\n",
      "[17] F004-1,5x.ppm\n",
      "[55] F039-1,5x.ppm\n",
      "[88] F061-1,5x.ppm\n",
      "[42] F080-G-1,5x.ppm\n",
      "[19] F026-1,5x.ppm\n",
      "[87] F064-1,5x.ppm\n",
      "[20] F028-1,5x.ppm\n",
      "[97] F056-1,5x.ppm\n",
      "[94] F091-G-1,5x.ppm\n",
      "[8] F048-1,5x.ppm\n",
      "[262] S001-1,5x.ppm\n",
      "[245] S003-1,5x.ppm\n",
      "[257] S004-1,5x.ppm\n",
      "[250] S005-1,5x.ppm\n",
      "[291] S006-1,5x.ppm\n",
      "[285] S007-1,5x.ppm\n",
      "[290] S008-1,5x.ppm\n",
      "[274] S009-1,5x.ppm\n",
      "[226] S010-1,5x.ppm\n",
      "[205] S011-1,5x.ppm\n",
      "[232] S012-1,5x.ppm\n",
      "[228] S015-1,5x.ppm\n",
      "[203] S016-1,5x.ppm\n",
      "[240] S018-1,5x.ppm\n",
      "[242] S019-1,5x.ppm\n",
      "[282] S020-1,5x.ppm\n",
      "[211] S021-1,5x.ppm\n",
      "[271] S022-1,5x.ppm\n",
      "[294] S023-1,5x.ppm\n",
      "[224] S024-1,5x.ppm\n",
      "[152] B077-G-1,5x.ppm\n",
      "[176] B074-G-1,5x.ppm\n",
      "[160] B056-1,5x.ppm\n",
      "[155] B061-M-1,5x.ppm\n",
      "[189] B020-1,5x.ppm\n",
      "[180] B015-1,5x.ppm\n",
      "[146] B101-G-1,5x.ppm\n",
      "[126] B082-G-1,5x.ppm\n",
      "[115] B093-G-1,5x.ppm\n",
      "[182] B042-1,5x.ppm\n",
      "[166] B062-G-1,5x.ppm\n",
      "[141] B002-1,5x.ppm\n",
      "[139] B033-15,x.ppm\n",
      "[130] B049-1,5x.ppm\n",
      "[145] B083-G-1,5x.ppm\n",
      "[138] B075-G-1,5x.ppm\n",
      "[112] B096-G-1,5x.ppm\n",
      "[135] B086-G-1,5x.ppm\n",
      "[173] B071-G-1,5x.ppm\n",
      "[101] B027-1,5x.ppm\n",
      "[11] F006-1,5x.ppm\n",
      "[66] F009-1,5x.ppm\n",
      "[68] F016,1,5x.ppm\n",
      "[10] F022-1,5x.ppm\n",
      "[33] F027-1,5x.ppm\n",
      "[30] F033-1,5x.ppm\n",
      "[39] F037-1,5x.ppm\n",
      "[67] F041-1,5x.ppm\n",
      "[77] F050-1,5x.ppm\n",
      "[96] F054-1,5x.ppm\n",
      "[41] F063-1,5x.ppm\n",
      "[23] F072-G-1,5x.ppm\n",
      "[83] F077-G-1,5x.ppm\n",
      "[18] F090-G-1,5x.ppm\n",
      "[44] F093-G-1,5x.ppm\n",
      "[15] F094-G-1,5x.ppm\n",
      "[63] F095-G-1,5x.ppm\n",
      "[99] F106-G-1,5x.ppm\n",
      "[53] F108-G-1,5x.ppm\n",
      "[47] F132-N-1,5x.ppm\n",
      "[254] S093-1,5x.ppm\n",
      "[216] S094-1,5x.ppm\n",
      "[288] S095-1,5x.ppm\n",
      "[213] S096-1,5x.ppm\n",
      "[279] S098-1,5x.ppm\n",
      "[207] S099-1,5x.ppm\n",
      "[209] S100-1,5x.ppm\n",
      "[259] S101-1,5x.ppm\n",
      "[300] s102-1,5x.ppm\n",
      "[268] S103-1,5x.ppm\n",
      "[278] S104-1,5x.ppm\n",
      "[251] S105-1,5x.ppm\n",
      "[267] S106-1,5x.ppm\n",
      "[281] S108-1,5x.ppm\n",
      "[244] S109-1,5x.ppm\n",
      "[229] S111-1,5x.ppm\n",
      "[247] S112-1,5x.ppm\n",
      "[208] S116-1,5x.ppm\n",
      "[261] S118-1,5x.ppm\n",
      "[234] S123-1,5x.ppm\n",
      "[131] B001-1,5x.ppm\n",
      "[193] B003,1,5x.ppm\n",
      "[140] B009-1,5x.ppm\n",
      "[167] B012-1,5X.ppm\n",
      "[195] B018-1,5x.ppm\n",
      "[162] B019-1,5x.ppm\n",
      "[158] B024-1,5x.ppm\n",
      "[110] B032-1,5x.ppm\n",
      "[137] B038-1,5x.ppm\n",
      "[134] B051-1,5x.ppm\n",
      "[129] B055-1,5x.ppm\n",
      "[194] B057-1,5x.ppm\n",
      "[153] B060-1,5x.ppm\n",
      "[150] B070-G-1,5x.ppm\n",
      "[174] B084-G-1,5x.ppm\n",
      "[100] B089-G-1,5x.ppm\n",
      "[127] B091-G-1,5x.ppm\n",
      "[106] B092-G-1,5x.ppm\n",
      "[149] B104-G-1,5x.ppm\n",
      "[142] B106-G-1,5x.ppm\n",
      "[175] B108-G-1,5x.ppm\n",
      "[51] F079-G-1,5x.ppm\n",
      "[98] F015-1,5x.ppm\n",
      "[14] F020-1,5x.ppm\n",
      "[54] F003-1,5x.ppm\n",
      "[61] F112-G-1,5x.ppm\n",
      "[50] F046-1,5x.ppm\n",
      "[86] F068-1,5x.ppm\n",
      "[80] F058-1,5x.ppm\n",
      "[48] F104-G-1,5x.ppm\n",
      "[72] F099-G-1,5x.ppm\n",
      "[6] F098-G-1,5x.ppm\n",
      "[9] F010-1,5x.ppm\n",
      "[13] F110-G-1,5x.ppm\n",
      "[79] F044-1,5x.ppm\n",
      "[65] F101-G-1,5x.ppm\n",
      "[75] F105-G-1,5x.ppm\n",
      "[70] F023-1,5x.ppm\n",
      "[38] F071-G-1,5x.ppm\n",
      "[24] F007-1,5x.ppm\n",
      "[4] F021-1,5x.ppm\n",
      "[266] S025-1,5x.ppm\n",
      "[236] S026-1,5x.ppm\n",
      "[218] S027-1,5x.ppm\n",
      "[299] S028-1,5x.ppm\n",
      "[243] S029-1,5x.ppm\n",
      "[289] S030-1,5x.ppm\n",
      "[284] S031-1,5x.ppm\n",
      "[286] S032-1,5x.ppm\n",
      "[201] S033-1,5x.ppm\n",
      "[221] S034-1,5x.ppm\n",
      "[253] S035-1,5x.ppm\n",
      "[214] S036-1,5x.ppm\n",
      "[276] S037-1,5x.ppm\n",
      "[204] S038-1,5x.ppm\n",
      "[256] S039-1,5x.ppm\n",
      "[230] S040-1,5x.ppm\n",
      "[202] S041-1,5x.ppm\n",
      "[219] S042-1,5x.ppm\n",
      "[206] S043-1,5x.ppm\n",
      "[298] S044-1,5x.ppm\n",
      "[104] B014-1,5x.ppm\n",
      "[125] B068-G-1,5x.ppm\n",
      "[196] B072-G-1,5x.ppm\n",
      "[188] B099-G-1,5x.ppm\n",
      "[103] B107-G-1,5x.ppm\n",
      "[151] B103-G-1,5x.ppm\n",
      "[108] B081-G-1,5x.ppm\n",
      "[172] B078-G-1,5x.ppm\n",
      "[144] B048-1,5x.ppm\n",
      "[133] B085-G-1,5x.ppm\n",
      "[148] B090-G-1,5x.ppm\n",
      "[191] B098-G-1,5x.ppm\n",
      "[157] B102-G-1,5x.ppm\n",
      "[169] B043-1,5x.ppm\n",
      "[168] B065-G-1,5x.ppm\n",
      "[163] B045-1,5x.ppm\n",
      "[177] B105_G-1,5x.ppm\n",
      "[198] B029-1,5x.ppm\n",
      "[109] B054-1,5x.ppm\n",
      "[113] B094-G-1,5x.ppm\n",
      "[59] F066-1,5x.ppm\n",
      "[7] F115-G-1,5x.ppm\n",
      "[73] F051-1,5x.ppm\n",
      "[64] F047-1,5x.ppm\n",
      "[57] F076-G-1,5x.ppm\n",
      "[82] F114-G-1,5x.ppm\n",
      "[78] F092-G-1,5x.ppm\n",
      "[40] F088-G-1,5x.ppm\n",
      "[90] F012-1,5x.ppm\n",
      "[27] F083-G-1,5x.ppm\n",
      "[31] F078-G-1,5x.ppm\n",
      "[5] F042-1,5x.ppm\n",
      "[52] F100-G-1,5x.ppm\n",
      "[32] F014-1,5x.ppm\n",
      "[28] F102-G-1,5x.ppm\n",
      "[26] F031-1,5x.ppm\n",
      "[37] F040-1,5x.ppm\n",
      "[60] F111-G-1,5x.ppm\n",
      "[21] F052-1,5x.ppm\n",
      "[58] F109-G-1,5x.ppm\n",
      "[238] S067-1,5x.ppm\n",
      "[272] S068-1,5x.ppm\n",
      "[217] S069-1,5x.ppm\n",
      "[223] S071-1,5x.ppm\n",
      "[277] S072-1,5x.ppm\n",
      "[235] S073-1,5x.ppm\n",
      "[222] S074-1,5x.ppm\n",
      "[215] S075-1,5x.ppm\n",
      "[237] S076-1,5x.ppm\n",
      "[233] S078-1,5x.ppm\n",
      "[270] S079-1,5x.ppm\n",
      "[265] S080-1,5x.ppm\n",
      "[252] S081-1,5x.ppm\n",
      "[220] S082-1,5x.ppm\n",
      "[273] S083-1,5x.ppm\n",
      "[275] S087-1,5x.ppm\n",
      "[231] S088-1,5x.ppm\n",
      "[295] S090-1,5x.ppm\n",
      "[283] S091-1,5x.ppm\n",
      "[269] S092-1,5x.ppm\n",
      "[190] B013-1,5x.ppm\n",
      "[123] B006-1,5x.ppm\n",
      "[178] B041-1,5x.ppm\n",
      "[117] B059-M-1,5x.ppm\n",
      "[179] B004-1,5x.ppm\n",
      "[116] B050-1,5x.ppm\n",
      "[200] B088-G01,5x.ppm\n",
      "[128] B026-15x.ppm\n",
      "[185] B052-1,5x.ppm\n",
      "[159] B008-1,5x.ppm\n",
      "[184] B010-1,5x.ppm\n",
      "[111] B065-1,5x.ppm\n",
      "[170] B067-G-1,5x.ppm\n",
      "[118] B087-G-1,5x.ppm\n",
      "[171] B040-1,5x.ppm\n",
      "[107] B079-G-1,5x.ppm\n",
      "[192] B044-1,5x.ppm\n",
      "[114] B005-1,5x.ppm\n",
      "[161] B063-G-1,5x.ppm\n",
      "[181] B046-1,5x.ppm\n",
      "[3] Train fold size: 241\n",
      "[1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181]\n",
      "Datasets/Folds/Fold-Test4.txt\n",
      "[69] F086-G-1,5x.ppm\n",
      "[85] F057-1,5x.ppm\n",
      "[46] F059-1,5x.ppm\n",
      "[12] F055-1,5x.ppm\n",
      "[84] F085-G-1,5x.ppm\n",
      "[3] F062-1,5x.ppm\n",
      "[22] F103-G-1,5x.ppm\n",
      "[25] F043-1,5x.ppm\n",
      "[89] F029-1,5x.ppm\n",
      "[34] F087-G-1,5x.ppm\n",
      "[49] F005-1,5x.ppm\n",
      "[0] F035-1,5x.ppm\n",
      "[92] F017-1,5x.ppm\n",
      "[45] F036-1,5x.ppm\n",
      "[56] F030-1,5x.ppm\n",
      "[29] F089-G-1,5x.ppm\n",
      "[16] F011-1,5x.ppm\n",
      "[36] F053-1,5x.ppm\n",
      "[74] F045-1,5x.ppm\n",
      "[93] F082-G-1,5x.ppm\n",
      "[225] S045-1,5x.ppm\n",
      "[264] S046-1,5x.ppm\n",
      "[255] S048-1,5x.ppm\n",
      "[292] S049-1,5x.ppm\n",
      "[293] S050-1,5x.ppm\n",
      "[280] S051-1,5x.ppm\n",
      "[297] S052-1,5x.ppm\n",
      "[239] S053-1,5x.ppm\n",
      "[241] S054-1,5x.ppm\n",
      "[258] S055-1,5.ppm\n",
      "[212] S056-1,5x.ppm\n",
      "[296] S057-1,5x.ppm\n",
      "[249] S058-1,5x.ppm\n",
      "[210] S059-1,5x.ppm\n",
      "[263] S060-1,5x.ppm\n",
      "[260] S061-1,5x.ppm\n",
      "[248] S062-1,5x.ppm\n",
      "[246] S063-1,5x.ppm\n",
      "[227] S064-1,5x.ppm\n",
      "[287] S065-1,5x.ppm\n",
      "[119] B097-G-1,5x.ppm\n",
      "[164] B017-1,5x.ppm\n",
      "[124] B095-G-1,5x.ppm\n",
      "[122] B080-G-1,5x.ppm\n",
      "[197] B053-1,5x.ppm\n",
      "[132] B100-G-1,5x.ppm\n",
      "[121] B066-G-1,5x.ppm\n",
      "[102] B069-G-1,5x.ppm\n",
      "[136] B034-1,5x.ppm\n",
      "[199] B007-1,5x.ppm\n",
      "[165] B031-1,5x.ppm\n",
      "[187] B073-G-1,5x.ppm\n",
      "[143] B028-1,5x.ppm\n",
      "[154] B039-1,5x.ppm\n",
      "[105] B058-1,5x.ppm\n",
      "[147] B016-1,5x.ppm\n",
      "[186] B030-1,5x.ppm\n",
      "[156] B076-G-1,5x.ppm\n",
      "[183] B025-1,5x.ppm\n",
      "[120] B047-1,5x.ppm\n",
      "[3] Test fold size: 60\n",
      "[69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120]\n",
      "Datasets/Folds/Fold-Train5.txt\n",
      "[1] F025-1,5x.ppm\n",
      "[35] F107-G-1,5x.ppm\n",
      "[95] F013-1,5x.ppm\n",
      "[91] F034-1,5x.ppm\n",
      "[62] F113-G-1,5x.ppm\n",
      "[2] F084-G-1,5x.ppm\n",
      "[76] F049-1,5x.ppm\n",
      "[81] F060-1,5x.ppm\n",
      "[43] F075-G-1,5x.ppm\n",
      "[71] F097-G-1,5x.ppm\n",
      "[17] F004-1,5x.ppm\n",
      "[55] F039-1,5x.ppm\n",
      "[88] F061-1,5x.ppm\n",
      "[42] F080-G-1,5x.ppm\n",
      "[19] F026-1,5x.ppm\n",
      "[87] F064-1,5x.ppm\n",
      "[20] F028-1,5x.ppm\n",
      "[97] F056-1,5x.ppm\n",
      "[94] F091-G-1,5x.ppm\n",
      "[8] F048-1,5x.ppm\n",
      "[262] S001-1,5x.ppm\n",
      "[245] S003-1,5x.ppm\n",
      "[257] S004-1,5x.ppm\n",
      "[250] S005-1,5x.ppm\n",
      "[291] S006-1,5x.ppm\n",
      "[285] S007-1,5x.ppm\n",
      "[290] S008-1,5x.ppm\n",
      "[274] S009-1,5x.ppm\n",
      "[226] S010-1,5x.ppm\n",
      "[205] S011-1,5x.ppm\n",
      "[232] S012-1,5x.ppm\n",
      "[228] S015-1,5x.ppm\n",
      "[203] S016-1,5x.ppm\n",
      "[240] S018-1,5x.ppm\n",
      "[242] S019-1,5x.ppm\n",
      "[282] S020-1,5x.ppm\n",
      "[211] S021-1,5x.ppm\n",
      "[271] S022-1,5x.ppm\n",
      "[294] S023-1,5x.ppm\n",
      "[224] S024-1,5x.ppm\n",
      "[152] B077-G-1,5x.ppm\n",
      "[176] B074-G-1,5x.ppm\n",
      "[160] B056-1,5x.ppm\n",
      "[155] B061-M-1,5x.ppm\n",
      "[189] B020-1,5x.ppm\n",
      "[180] B015-1,5x.ppm\n",
      "[146] B101-G-1,5x.ppm\n",
      "[126] B082-G-1,5x.ppm\n",
      "[115] B093-G-1,5x.ppm\n",
      "[182] B042-1,5x.ppm\n",
      "[166] B062-G-1,5x.ppm\n",
      "[141] B002-1,5x.ppm\n",
      "[139] B033-15,x.ppm\n",
      "[130] B049-1,5x.ppm\n",
      "[145] B083-G-1,5x.ppm\n",
      "[138] B075-G-1,5x.ppm\n",
      "[112] B096-G-1,5x.ppm\n",
      "[135] B086-G-1,5x.ppm\n",
      "[173] B071-G-1,5x.ppm\n",
      "[101] B027-1,5x.ppm\n",
      "[11] F006-1,5x.ppm\n",
      "[66] F009-1,5x.ppm\n",
      "[68] F016,1,5x.ppm\n",
      "[10] F022-1,5x.ppm\n",
      "[33] F027-1,5x.ppm\n",
      "[30] F033-1,5x.ppm\n",
      "[39] F037-1,5x.ppm\n",
      "[67] F041-1,5x.ppm\n",
      "[77] F050-1,5x.ppm\n",
      "[96] F054-1,5x.ppm\n",
      "[41] F063-1,5x.ppm\n",
      "[23] F072-G-1,5x.ppm\n",
      "[83] F077-G-1,5x.ppm\n",
      "[18] F090-G-1,5x.ppm\n",
      "[44] F093-G-1,5x.ppm\n",
      "[15] F094-G-1,5x.ppm\n",
      "[63] F095-G-1,5x.ppm\n",
      "[99] F106-G-1,5x.ppm\n",
      "[53] F108-G-1,5x.ppm\n",
      "[47] F132-N-1,5x.ppm\n",
      "[254] S093-1,5x.ppm\n",
      "[216] S094-1,5x.ppm\n",
      "[288] S095-1,5x.ppm\n",
      "[213] S096-1,5x.ppm\n",
      "[279] S098-1,5x.ppm\n",
      "[207] S099-1,5x.ppm\n",
      "[209] S100-1,5x.ppm\n",
      "[259] S101-1,5x.ppm\n",
      "[300] s102-1,5x.ppm\n",
      "[268] S103-1,5x.ppm\n",
      "[278] S104-1,5x.ppm\n",
      "[251] S105-1,5x.ppm\n",
      "[267] S106-1,5x.ppm\n",
      "[281] S108-1,5x.ppm\n",
      "[244] S109-1,5x.ppm\n",
      "[229] S111-1,5x.ppm\n",
      "[247] S112-1,5x.ppm\n",
      "[208] S116-1,5x.ppm\n",
      "[261] S118-1,5x.ppm\n",
      "[234] S123-1,5x.ppm\n",
      "[131] B001-1,5x.ppm\n",
      "[193] B003,1,5x.ppm\n",
      "[140] B009-1,5x.ppm\n",
      "[167] B012-1,5X.ppm\n",
      "[195] B018-1,5x.ppm\n",
      "[162] B019-1,5x.ppm\n",
      "[158] B024-1,5x.ppm\n",
      "[110] B032-1,5x.ppm\n",
      "[137] B038-1,5x.ppm\n",
      "[134] B051-1,5x.ppm\n",
      "[129] B055-1,5x.ppm\n",
      "[194] B057-1,5x.ppm\n",
      "[153] B060-1,5x.ppm\n",
      "[150] B070-G-1,5x.ppm\n",
      "[174] B084-G-1,5x.ppm\n",
      "[100] B089-G-1,5x.ppm\n",
      "[127] B091-G-1,5x.ppm\n",
      "[106] B092-G-1,5x.ppm\n",
      "[149] B104-G-1,5x.ppm\n",
      "[142] B106-G-1,5x.ppm\n",
      "[175] B108-G-1,5x.ppm\n",
      "[51] F079-G-1,5x.ppm\n",
      "[98] F015-1,5x.ppm\n",
      "[14] F020-1,5x.ppm\n",
      "[54] F003-1,5x.ppm\n",
      "[61] F112-G-1,5x.ppm\n",
      "[50] F046-1,5x.ppm\n",
      "[86] F068-1,5x.ppm\n",
      "[80] F058-1,5x.ppm\n",
      "[48] F104-G-1,5x.ppm\n",
      "[72] F099-G-1,5x.ppm\n",
      "[6] F098-G-1,5x.ppm\n",
      "[9] F010-1,5x.ppm\n",
      "[13] F110-G-1,5x.ppm\n",
      "[79] F044-1,5x.ppm\n",
      "[65] F101-G-1,5x.ppm\n",
      "[75] F105-G-1,5x.ppm\n",
      "[70] F023-1,5x.ppm\n",
      "[38] F071-G-1,5x.ppm\n",
      "[24] F007-1,5x.ppm\n",
      "[4] F021-1,5x.ppm\n",
      "[266] S025-1,5x.ppm\n",
      "[236] S026-1,5x.ppm\n",
      "[218] S027-1,5x.ppm\n",
      "[299] S028-1,5x.ppm\n",
      "[243] S029-1,5x.ppm\n",
      "[289] S030-1,5x.ppm\n",
      "[284] S031-1,5x.ppm\n",
      "[286] S032-1,5x.ppm\n",
      "[201] S033-1,5x.ppm\n",
      "[221] S034-1,5x.ppm\n",
      "[253] S035-1,5x.ppm\n",
      "[214] S036-1,5x.ppm\n",
      "[276] S037-1,5x.ppm\n",
      "[204] S038-1,5x.ppm\n",
      "[256] S039-1,5x.ppm\n",
      "[230] S040-1,5x.ppm\n",
      "[202] S041-1,5x.ppm\n",
      "[219] S042-1,5x.ppm\n",
      "[206] S043-1,5x.ppm\n",
      "[298] S044-1,5x.ppm\n",
      "[104] B014-1,5x.ppm\n",
      "[125] B068-G-1,5x.ppm\n",
      "[196] B072-G-1,5x.ppm\n",
      "[188] B099-G-1,5x.ppm\n",
      "[103] B107-G-1,5x.ppm\n",
      "[151] B103-G-1,5x.ppm\n",
      "[108] B081-G-1,5x.ppm\n",
      "[172] B078-G-1,5x.ppm\n",
      "[144] B048-1,5x.ppm\n",
      "[133] B085-G-1,5x.ppm\n",
      "[148] B090-G-1,5x.ppm\n",
      "[191] B098-G-1,5x.ppm\n",
      "[157] B102-G-1,5x.ppm\n",
      "[169] B043-1,5x.ppm\n",
      "[168] B065-G-1,5x.ppm\n",
      "[163] B045-1,5x.ppm\n",
      "[177] B105_G-1,5x.ppm\n",
      "[198] B029-1,5x.ppm\n",
      "[109] B054-1,5x.ppm\n",
      "[113] B094-G-1,5x.ppm\n",
      "[69] F086-G-1,5x.ppm\n",
      "[85] F057-1,5x.ppm\n",
      "[46] F059-1,5x.ppm\n",
      "[12] F055-1,5x.ppm\n",
      "[84] F085-G-1,5x.ppm\n",
      "[3] F062-1,5x.ppm\n",
      "[22] F103-G-1,5x.ppm\n",
      "[25] F043-1,5x.ppm\n",
      "[89] F029-1,5x.ppm\n",
      "[34] F087-G-1,5x.ppm\n",
      "[49] F005-1,5x.ppm\n",
      "[0] F035-1,5x.ppm\n",
      "[92] F017-1,5x.ppm\n",
      "[45] F036-1,5x.ppm\n",
      "[56] F030-1,5x.ppm\n",
      "[29] F089-G-1,5x.ppm\n",
      "[16] F011-1,5x.ppm\n",
      "[36] F053-1,5x.ppm\n",
      "[74] F045-1,5x.ppm\n",
      "[93] F082-G-1,5x.ppm\n",
      "[225] S045-1,5x.ppm\n",
      "[264] S046-1,5x.ppm\n",
      "[255] S048-1,5x.ppm\n",
      "[292] S049-1,5x.ppm\n",
      "[293] S050-1,5x.ppm\n",
      "[280] S051-1,5x.ppm\n",
      "[297] S052-1,5x.ppm\n",
      "[239] S053-1,5x.ppm\n",
      "[241] S054-1,5x.ppm\n",
      "[258] S055-1,5.ppm\n",
      "[212] S056-1,5x.ppm\n",
      "[296] S057-1,5x.ppm\n",
      "[249] S058-1,5x.ppm\n",
      "[210] S059-1,5x.ppm\n",
      "[263] S060-1,5x.ppm\n",
      "[260] S061-1,5x.ppm\n",
      "[248] S062-1,5x.ppm\n",
      "[246] S063-1,5x.ppm\n",
      "[227] S064-1,5x.ppm\n",
      "[287] S065-1,5x.ppm\n",
      "[119] B097-G-1,5x.ppm\n",
      "[164] B017-1,5x.ppm\n",
      "[124] B095-G-1,5x.ppm\n",
      "[122] B080-G-1,5x.ppm\n",
      "[197] B053-1,5x.ppm\n",
      "[132] B100-G-1,5x.ppm\n",
      "[121] B066-G-1,5x.ppm\n",
      "[102] B069-G-1,5x.ppm\n",
      "[136] B034-1,5x.ppm\n",
      "[199] B007-1,5x.ppm\n",
      "[165] B031-1,5x.ppm\n",
      "[187] B073-G-1,5x.ppm\n",
      "[143] B028-1,5x.ppm\n",
      "[154] B039-1,5x.ppm\n",
      "[105] B058-1,5x.ppm\n",
      "[147] B016-1,5x.ppm\n",
      "[186] B030-1,5x.ppm\n",
      "[156] B076-G-1,5x.ppm\n",
      "[183] B025-1,5x.ppm\n",
      "[120] B047-1,5x.ppm\n",
      "[4] Train fold size: 241\n",
      "[1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120]\n",
      "Datasets/Folds/Fold-Test5.txt\n",
      "[59] F066-1,5x.ppm\n",
      "[7] F115-G-1,5x.ppm\n",
      "[73] F051-1,5x.ppm\n",
      "[64] F047-1,5x.ppm\n",
      "[57] F076-G-1,5x.ppm\n",
      "[82] F114-G-1,5x.ppm\n",
      "[78] F092-G-1,5x.ppm\n",
      "[40] F088-G-1,5x.ppm\n",
      "[90] F012-1,5x.ppm\n",
      "[27] F083-G-1,5x.ppm\n",
      "[31] F078-G-1,5x.ppm\n",
      "[5] F042-1,5x.ppm\n",
      "[52] F100-G-1,5x.ppm\n",
      "[32] F014-1,5x.ppm\n",
      "[28] F102-G-1,5x.ppm\n",
      "[26] F031-1,5x.ppm\n",
      "[37] F040-1,5x.ppm\n",
      "[60] F111-G-1,5x.ppm\n",
      "[21] F052-1,5x.ppm\n",
      "[58] F109-G-1,5x.ppm\n",
      "[238] S067-1,5x.ppm\n",
      "[272] S068-1,5x.ppm\n",
      "[217] S069-1,5x.ppm\n",
      "[223] S071-1,5x.ppm\n",
      "[277] S072-1,5x.ppm\n",
      "[235] S073-1,5x.ppm\n",
      "[222] S074-1,5x.ppm\n",
      "[215] S075-1,5x.ppm\n",
      "[237] S076-1,5x.ppm\n",
      "[233] S078-1,5x.ppm\n",
      "[270] S079-1,5x.ppm\n",
      "[265] S080-1,5x.ppm\n",
      "[252] S081-1,5x.ppm\n",
      "[220] S082-1,5x.ppm\n",
      "[273] S083-1,5x.ppm\n",
      "[275] S087-1,5x.ppm\n",
      "[231] S088-1,5x.ppm\n",
      "[295] S090-1,5x.ppm\n",
      "[283] S091-1,5x.ppm\n",
      "[269] S092-1,5x.ppm\n",
      "[190] B013-1,5x.ppm\n",
      "[123] B006-1,5x.ppm\n",
      "[178] B041-1,5x.ppm\n",
      "[117] B059-M-1,5x.ppm\n",
      "[179] B004-1,5x.ppm\n",
      "[116] B050-1,5x.ppm\n",
      "[200] B088-G01,5x.ppm\n",
      "[128] B026-15x.ppm\n",
      "[185] B052-1,5x.ppm\n",
      "[159] B008-1,5x.ppm\n",
      "[184] B010-1,5x.ppm\n",
      "[111] B065-1,5x.ppm\n",
      "[170] B067-G-1,5x.ppm\n",
      "[118] B087-G-1,5x.ppm\n",
      "[171] B040-1,5x.ppm\n",
      "[107] B079-G-1,5x.ppm\n",
      "[192] B044-1,5x.ppm\n",
      "[114] B005-1,5x.ppm\n",
      "[161] B063-G-1,5x.ppm\n",
      "[181] B046-1,5x.ppm\n",
      "[4] Test fold size: 60\n",
      "[59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181]\n",
      "SKFIND\n",
      "[([11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181], [1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101]), ([1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181], [11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175]), ([1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181], [51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113]), ([1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181], [69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120]), ([1, 35, 95, 91, 62, 2, 76, 81, 43, 71, 17, 55, 88, 42, 19, 87, 20, 97, 94, 8, 262, 245, 257, 250, 291, 285, 290, 274, 226, 205, 232, 228, 203, 240, 242, 282, 211, 271, 294, 224, 152, 176, 160, 155, 189, 180, 146, 126, 115, 182, 166, 141, 139, 130, 145, 138, 112, 135, 173, 101, 11, 66, 68, 10, 33, 30, 39, 67, 77, 96, 41, 23, 83, 18, 44, 15, 63, 99, 53, 47, 254, 216, 288, 213, 279, 207, 209, 259, 300, 268, 278, 251, 267, 281, 244, 229, 247, 208, 261, 234, 131, 193, 140, 167, 195, 162, 158, 110, 137, 134, 129, 194, 153, 150, 174, 100, 127, 106, 149, 142, 175, 51, 98, 14, 54, 61, 50, 86, 80, 48, 72, 6, 9, 13, 79, 65, 75, 70, 38, 24, 4, 266, 236, 218, 299, 243, 289, 284, 286, 201, 221, 253, 214, 276, 204, 256, 230, 202, 219, 206, 298, 104, 125, 196, 188, 103, 151, 108, 172, 144, 133, 148, 191, 157, 169, 168, 163, 177, 198, 109, 113, 69, 85, 46, 12, 84, 3, 22, 25, 89, 34, 49, 0, 92, 45, 56, 29, 16, 36, 74, 93, 225, 264, 255, 292, 293, 280, 297, 239, 241, 258, 212, 296, 249, 210, 263, 260, 248, 246, 227, 287, 119, 164, 124, 122, 197, 132, 121, 102, 136, 199, 165, 187, 143, 154, 105, 147, 186, 156, 183, 120], [59, 7, 73, 64, 57, 82, 78, 40, 90, 27, 31, 5, 52, 32, 28, 26, 37, 60, 21, 58, 238, 272, 217, 223, 277, 235, 222, 215, 237, 233, 270, 265, 252, 220, 273, 275, 231, 295, 283, 269, 190, 123, 178, 117, 179, 116, 200, 128, 185, 159, 184, 111, 170, 118, 171, 107, 192, 114, 161, 181])]\n",
      "TEST_FOLD\n",
      "[3 0 0 3 2 4 2 4 0 2 1 1 3 2 2 1 3 0 1 0 0 4 3 1 2 3 4 4 4 3 1 4 4 1 3 0 3\n",
      " 4 2 1 4 1 0 0 1 3 3 1 2 3 2 2 4 1 2 0 3 4 4 4 4 2 0 1 4 2 1 1 1 3 2 0 2 4\n",
      " 3 2 0 1 4 2 2 0 4 1 3 3 2 0 0 3 4 0 3 3 0 0 1 0 2 1 1 0 3 2 2 3 1 4 2 2 1\n",
      " 4 0 2 4 0 4 4 4 3 3 3 3 4 3 2 0 1 4 1 0 1 3 2 1 0 3 1 0 0 1 0 1 3 2 0 0 3\n",
      " 2 1 1 2 0 1 3 0 3 2 1 4 0 4 1 2 3 3 0 1 2 2 4 4 2 0 1 1 0 2 4 4 0 4 0 3 4\n",
      " 4 3 3 2 0 4 2 4 1 1 1 2 3 2 3 4 2 2 0 2 0 2 1 1 1 3 0 3 1 2 4 1 4 2 2 4 2\n",
      " 4 4 0 3 0 3 0 1 2 4 0 4 1 4 2 4 4 3 0 3 0 2 1 0 3 1 3 3 0 1 4 2 1 3 2 0 3\n",
      " 1 3 1 0 3 3 4 2 1 1 4 4 0 4 4 0 4 2 4 1 1 3 1 0 4 2 0 2 3 1 2 0 0 3 3 0 4\n",
      " 3 3 2 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Create stratified k-fold subsets                                                                                                                                        \n",
    "kfold = 5  # no. of folds\n",
    "skfind = []\n",
    "test_fold = np.zeros((num_samples,), dtype=int)-1\n",
    "for i in range(kfold):\n",
    "    train_ind = []\n",
    "    print('Datasets/Folds/Fold-Train%d.txt' %(i+1))\n",
    "    with open('Datasets/Folds/Fold-Train%d.txt' %(i+1), 'r') as f:\n",
    "        for fname in f.read().splitlines():\n",
    "            print('[%d] %s' %(list_paths.index([s for s in list_paths if fname in s][0]),fname))\n",
    "            train_ind.append(list_paths.index([s for s in list_paths if fname in s][0]))\n",
    "    print('[%d] Train fold size: %d' %(i,len(train_ind)))\n",
    "    print(train_ind)\n",
    "    test_ind = []\n",
    "    print('Datasets/Folds/Fold-Test%d.txt' %(i+1))\n",
    "    with open('Datasets/Folds/Fold-Test%d.txt' %(i+1), 'r') as f:\n",
    "        for fname in f.read().splitlines():\n",
    "            sample_idx = list_paths.index([s for s in list_paths if fname in s][0])\n",
    "            print('[%d] %s' %(sample_idx,fname))\n",
    "            test_ind.append(sample_idx)\n",
    "            test_fold[sample_idx] = i\n",
    "    print('[%d] Test fold size: %d' %(i,len(test_ind)))\n",
    "    print(test_ind)\n",
    "    #skfind.append((np.array(train_ind),np.array(test_ind)))\n",
    "    skfind.append((train_ind,test_ind))\n",
    "print('SKFIND')\n",
    "print(skfind)\n",
    "print('TEST_FOLD')\n",
    "print(test_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating base_model (ResNet50 notop)\n",
    "image_shape = (224, 224, 3)                                                                                                                                                                                                                                                                                            \n",
    "base_model = ResNet50(weights='imagenet', input_shape=image_shape, include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from ResNet50 layers ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features from ResNet50 layers ...\")\n",
    "resnet50features = base_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.38027704  0.          0.20684466 ...,  1.12184477  0.89628559\n",
      "   0.49559101]\n",
      " [ 0.42370874  0.0989799   0.0193436  ...,  0.74370098  0.51598448\n",
      "   0.10375948]\n",
      " [ 0.26371157  0.10568655  0.74503374 ...,  0.64799303  0.80613029\n",
      "   0.42094493]\n",
      " ..., \n",
      " [ 0.72277623  0.          0.         ...,  0.96262097  0.46300632\n",
      "   0.04655574]\n",
      " [ 0.76662195  0.08702008  0.0531584  ...,  1.37098396  0.24310508\n",
      "   0.23017983]\n",
      " [ 0.43618611  0.00297298  0.17552026 ...,  1.50332224  0.90540612\n",
      "   0.29464975]]\n"
     ]
    }
   ],
   "source": [
    "print(resnet50features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 2048)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 224, 224, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 112, 112, 64)  9472        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bn_conv1 (BatchNormalization)    (None, 112, 112, 64)  256         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 112, 112, 64)  0           bn_conv1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, 55, 55, 64)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2a (Conv2D)          (None, 55, 55, 64)    4160        max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 55, 55, 64)    0           bn2a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 55, 55, 64)    0           bn2a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "res2a_branch1 (Conv2D)           (None, 55, 55, 256)   16640       max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn2a_branch1 (BatchNormalization (None, 55, 55, 256)   1024        res2a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 55, 55, 256)   0           bn2a_branch2c[0][0]              \n",
      "                                                                   bn2a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 55, 55, 256)   0           add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2a (Conv2D)          (None, 55, 55, 64)    16448       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 55, 55, 64)    0           bn2b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 55, 55, 64)    0           bn2b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2b_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2b_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 55, 55, 256)   0           bn2b_branch2c[0][0]              \n",
      "                                                                   activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 55, 55, 256)   0           add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2a (Conv2D)          (None, 55, 55, 64)    16448       activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2a (BatchNormalizatio (None, 55, 55, 64)    256         res2c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 55, 55, 64)    0           bn2c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2b (Conv2D)          (None, 55, 55, 64)    36928       activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2b (BatchNormalizatio (None, 55, 55, 64)    256         res2c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, 55, 55, 64)    0           bn2c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res2c_branch2c (Conv2D)          (None, 55, 55, 256)   16640       activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "bn2c_branch2c (BatchNormalizatio (None, 55, 55, 256)   1024        res2c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 55, 55, 256)   0           bn2c_branch2c[0][0]              \n",
      "                                                                   activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 55, 55, 256)   0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2a (Conv2D)          (None, 28, 28, 128)   32896       activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 28, 28, 128)   0           bn3a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 28, 28, 128)   0           bn3a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3a_branch1 (Conv2D)           (None, 28, 28, 512)   131584      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn3a_branch1 (BatchNormalization (None, 28, 28, 512)   2048        res3a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_4 (Add)                      (None, 28, 28, 512)   0           bn3a_branch2c[0][0]              \n",
      "                                                                   bn3a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 28, 28, 512)   0           add_4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 28, 28, 128)   0           bn3b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 28, 28, 128)   0           bn3b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3b_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3b_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_5 (Add)                      (None, 28, 28, 512)   0           bn3b_branch2c[0][0]              \n",
      "                                                                   activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 28, 28, 512)   0           add_5[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 28, 28, 128)   0           bn3c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, 28, 28, 128)   0           bn3c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3c_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3c_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_6 (Add)                      (None, 28, 28, 512)   0           bn3c_branch2c[0][0]              \n",
      "                                                                   activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, 28, 28, 512)   0           add_6[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2a (Conv2D)          (None, 28, 28, 128)   65664       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2a (BatchNormalizatio (None, 28, 28, 128)   512         res3d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, 28, 28, 128)   0           bn3d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2b (Conv2D)          (None, 28, 28, 128)   147584      activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2b (BatchNormalizatio (None, 28, 28, 128)   512         res3d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 28, 28, 128)   0           bn3d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res3d_branch2c (Conv2D)          (None, 28, 28, 512)   66048       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn3d_branch2c (BatchNormalizatio (None, 28, 28, 512)   2048        res3d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_7 (Add)                      (None, 28, 28, 512)   0           bn3d_branch2c[0][0]              \n",
      "                                                                   activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 28, 28, 512)   0           add_7[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2a (Conv2D)          (None, 14, 14, 256)   131328      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, 14, 14, 256)   0           bn4a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_24 (Activation)       (None, 14, 14, 256)   0           bn4a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_24[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4a_branch1 (Conv2D)           (None, 14, 14, 1024)  525312      activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn4a_branch1 (BatchNormalization (None, 14, 14, 1024)  4096        res4a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_8 (Add)                      (None, 14, 14, 1024)  0           bn4a_branch2c[0][0]              \n",
      "                                                                   bn4a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 14, 14, 1024)  0           add_8[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 14, 14, 256)   0           bn4b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 14, 14, 256)   0           bn4b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4b_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4b_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_9 (Add)                      (None, 14, 14, 1024)  0           bn4b_branch2c[0][0]              \n",
      "                                                                   activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 14, 14, 1024)  0           add_9[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 14, 14, 256)   0           bn4c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 14, 14, 256)   0           bn4c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4c_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4c_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_10 (Add)                     (None, 14, 14, 1024)  0           bn4c_branch2c[0][0]              \n",
      "                                                                   activation_28[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 14, 14, 1024)  0           add_10[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4d_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 14, 14, 256)   0           bn4d_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_32[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4d_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_33 (Activation)       (None, 14, 14, 256)   0           bn4d_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4d_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_33[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4d_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4d_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_11 (Add)                     (None, 14, 14, 1024)  0           bn4d_branch2c[0][0]              \n",
      "                                                                   activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_34 (Activation)       (None, 14, 14, 1024)  0           add_11[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4e_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_35 (Activation)       (None, 14, 14, 256)   0           bn4e_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_35[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4e_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_36 (Activation)       (None, 14, 14, 256)   0           bn4e_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4e_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_36[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4e_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4e_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_12 (Add)                     (None, 14, 14, 1024)  0           bn4e_branch2c[0][0]              \n",
      "                                                                   activation_34[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_37 (Activation)       (None, 14, 14, 1024)  0           add_12[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2a (Conv2D)          (None, 14, 14, 256)   262400      activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2a (BatchNormalizatio (None, 14, 14, 256)   1024        res4f_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_38 (Activation)       (None, 14, 14, 256)   0           bn4f_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2b (Conv2D)          (None, 14, 14, 256)   590080      activation_38[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2b (BatchNormalizatio (None, 14, 14, 256)   1024        res4f_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_39 (Activation)       (None, 14, 14, 256)   0           bn4f_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res4f_branch2c (Conv2D)          (None, 14, 14, 1024)  263168      activation_39[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn4f_branch2c (BatchNormalizatio (None, 14, 14, 1024)  4096        res4f_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_13 (Add)                     (None, 14, 14, 1024)  0           bn4f_branch2c[0][0]              \n",
      "                                                                   activation_37[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_40 (Activation)       (None, 14, 14, 1024)  0           add_13[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2a (Conv2D)          (None, 7, 7, 512)     524800      activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_41 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_41[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5a_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_42 (Activation)       (None, 7, 7, 512)     0           bn5a_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_42[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5a_branch1 (Conv2D)           (None, 7, 7, 2048)    2099200     activation_40[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5a_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "bn5a_branch1 (BatchNormalization (None, 7, 7, 2048)    8192        res5a_branch1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "add_14 (Add)                     (None, 7, 7, 2048)    0           bn5a_branch2c[0][0]              \n",
      "                                                                   bn5a_branch1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_43 (Activation)       (None, 7, 7, 2048)    0           add_14[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_44 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_44[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5b_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_45 (Activation)       (None, 7, 7, 512)     0           bn5b_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5b_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_45[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5b_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5b_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_15 (Add)                     (None, 7, 7, 2048)    0           bn5b_branch2c[0][0]              \n",
      "                                                                   activation_43[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_46 (Activation)       (None, 7, 7, 2048)    0           add_15[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2a (Conv2D)          (None, 7, 7, 512)     1049088     activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2a (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2a[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_47 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2a[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2b (Conv2D)          (None, 7, 7, 512)     2359808     activation_47[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2b (BatchNormalizatio (None, 7, 7, 512)     2048        res5c_branch2b[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_48 (Activation)       (None, 7, 7, 512)     0           bn5c_branch2b[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "res5c_branch2c (Conv2D)          (None, 7, 7, 2048)    1050624     activation_48[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "bn5c_branch2c (BatchNormalizatio (None, 7, 7, 2048)    8192        res5c_branch2c[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "add_16 (Add)                     (None, 7, 7, 2048)    0           bn5c_branch2c[0][0]              \n",
      "                                                                   activation_46[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_49 (Activation)       (None, 7, 7, 2048)    0           add_16[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0           activation_49[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glob (None, 2048)          0           avg_pool[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 3)             6147        global_average_pooling2d_1[0][0] \n",
      "====================================================================================================\n",
      "Total params: 23,593,859\n",
      "Trainable params: 23,540,739\n",
      "Non-trainable params: 53,120\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Adding a custom top layer to the model\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(base_model.output)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "#model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bottleneck_features (InputLa (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 3)                 6147      \n",
      "=================================================================\n",
      "Total params: 6,147\n",
      "Trainable params: 6,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the top model for pre-training\n",
    "bottleneck_features = Input(shape=resnet50features.shape[1:], name='bottleneck_features')\n",
    "predictions = Dense(num_classes, activation='softmax', name='predictions')(bottleneck_features)\n",
    "top_model = Model(inputs=bottleneck_features, outputs=predictions)\n",
    "top_model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning the model (base_model + top layer) \n",
    "num_epochs=200\n",
    "history = []\n",
    "tophistory = []\n",
    "conf_mat = np.zeros((len(list_fams),len(list_fams))) # Initializing the Confusion Matrix\n",
    "checkpointer = ModelCheckpoint(filepath='moscas-folds-resnet50-pretrained-finetune.h5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "callbacks_list = [checkpointer]\n",
    "init_weights = model.get_weights()\n",
    "init_top_weights = top_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Pre-training the top model ...\n",
      "[0] Initial acurracy (top_model): 0.3000\n",
      "Train on 241 samples, validate on 60 samples\n",
      "Epoch 1/200\n",
      "241/241 [==============================] - 0s - loss: 1.1766 - acc: 0.3610 - val_loss: 0.9509 - val_acc: 0.6333\n",
      "Epoch 2/200\n",
      "241/241 [==============================] - 0s - loss: 1.0015 - acc: 0.4896 - val_loss: 0.8415 - val_acc: 0.6167\n",
      "Epoch 3/200\n",
      "241/241 [==============================] - 0s - loss: 0.8908 - acc: 0.6141 - val_loss: 0.7806 - val_acc: 0.6500\n",
      "Epoch 4/200\n",
      "241/241 [==============================] - 0s - loss: 0.7766 - acc: 0.6888 - val_loss: 0.6726 - val_acc: 0.7667\n",
      "Epoch 5/200\n",
      "241/241 [==============================] - 0s - loss: 0.6872 - acc: 0.7469 - val_loss: 0.6007 - val_acc: 0.8167\n",
      "Epoch 6/200\n",
      "241/241 [==============================] - 0s - loss: 0.6274 - acc: 0.8091 - val_loss: 0.5718 - val_acc: 0.8167\n",
      "Epoch 7/200\n",
      "241/241 [==============================] - 0s - loss: 0.5484 - acc: 0.8506 - val_loss: 0.5152 - val_acc: 0.8333\n",
      "Epoch 8/200\n",
      "241/241 [==============================] - 0s - loss: 0.5212 - acc: 0.8589 - val_loss: 0.4847 - val_acc: 0.8333\n",
      "Epoch 9/200\n",
      "241/241 [==============================] - 0s - loss: 0.4777 - acc: 0.8714 - val_loss: 0.4873 - val_acc: 0.8500\n",
      "Epoch 10/200\n",
      "241/241 [==============================] - 0s - loss: 0.4458 - acc: 0.8838 - val_loss: 0.4448 - val_acc: 0.8333\n",
      "Epoch 11/200\n",
      "241/241 [==============================] - 0s - loss: 0.4174 - acc: 0.8880 - val_loss: 0.4288 - val_acc: 0.8500\n",
      "Epoch 12/200\n",
      "241/241 [==============================] - 0s - loss: 0.3862 - acc: 0.9087 - val_loss: 0.4336 - val_acc: 0.8333\n",
      "Epoch 13/200\n",
      "241/241 [==============================] - 0s - loss: 0.3633 - acc: 0.9087 - val_loss: 0.4089 - val_acc: 0.8500\n",
      "Epoch 14/200\n",
      "241/241 [==============================] - 0s - loss: 0.3456 - acc: 0.9170 - val_loss: 0.3947 - val_acc: 0.8500\n",
      "Epoch 15/200\n",
      "241/241 [==============================] - 0s - loss: 0.3327 - acc: 0.9129 - val_loss: 0.3982 - val_acc: 0.8333\n",
      "Epoch 16/200\n",
      "241/241 [==============================] - 0s - loss: 0.3142 - acc: 0.9295 - val_loss: 0.3989 - val_acc: 0.8500\n",
      "Epoch 17/200\n",
      "241/241 [==============================] - 0s - loss: 0.2971 - acc: 0.9336 - val_loss: 0.3838 - val_acc: 0.8333\n",
      "Epoch 18/200\n",
      "241/241 [==============================] - 0s - loss: 0.2904 - acc: 0.9253 - val_loss: 0.3737 - val_acc: 0.8333\n",
      "Epoch 19/200\n",
      "241/241 [==============================] - 0s - loss: 0.2756 - acc: 0.9461 - val_loss: 0.3796 - val_acc: 0.8667\n",
      "Epoch 20/200\n",
      "241/241 [==============================] - 0s - loss: 0.2599 - acc: 0.9502 - val_loss: 0.3661 - val_acc: 0.8333\n",
      "Epoch 21/200\n",
      "241/241 [==============================] - 0s - loss: 0.2488 - acc: 0.9419 - val_loss: 0.3623 - val_acc: 0.8333\n",
      "Epoch 22/200\n",
      "241/241 [==============================] - 0s - loss: 0.2415 - acc: 0.9419 - val_loss: 0.3474 - val_acc: 0.8333\n",
      "Epoch 23/200\n",
      "241/241 [==============================] - 0s - loss: 0.2277 - acc: 0.9585 - val_loss: 0.3632 - val_acc: 0.8500\n",
      "Epoch 24/200\n",
      "241/241 [==============================] - 0s - loss: 0.2212 - acc: 0.9585 - val_loss: 0.3570 - val_acc: 0.8333\n",
      "Epoch 25/200\n",
      "241/241 [==============================] - 0s - loss: 0.2106 - acc: 0.9668 - val_loss: 0.3434 - val_acc: 0.8333\n",
      "Epoch 26/200\n",
      "241/241 [==============================] - 0s - loss: 0.2051 - acc: 0.9627 - val_loss: 0.3364 - val_acc: 0.8500\n",
      "Epoch 27/200\n",
      "241/241 [==============================] - 0s - loss: 0.1961 - acc: 0.9668 - val_loss: 0.3541 - val_acc: 0.8333\n",
      "Epoch 28/200\n",
      "241/241 [==============================] - 0s - loss: 0.1909 - acc: 0.9668 - val_loss: 0.3440 - val_acc: 0.8333\n",
      "Epoch 29/200\n",
      "241/241 [==============================] - 0s - loss: 0.1839 - acc: 0.9710 - val_loss: 0.3258 - val_acc: 0.8500\n",
      "Epoch 30/200\n",
      "241/241 [==============================] - 0s - loss: 0.1785 - acc: 0.9793 - val_loss: 0.3427 - val_acc: 0.8500\n",
      "Epoch 31/200\n",
      "241/241 [==============================] - 0s - loss: 0.1730 - acc: 0.9793 - val_loss: 0.3343 - val_acc: 0.8333\n",
      "Epoch 32/200\n",
      "241/241 [==============================] - 0s - loss: 0.1671 - acc: 0.9834 - val_loss: 0.3304 - val_acc: 0.8333\n",
      "Epoch 33/200\n",
      "241/241 [==============================] - 0s - loss: 0.1617 - acc: 0.9876 - val_loss: 0.3213 - val_acc: 0.8667\n",
      "Epoch 34/200\n",
      "241/241 [==============================] - 0s - loss: 0.1578 - acc: 0.9834 - val_loss: 0.3295 - val_acc: 0.8500\n",
      "Epoch 35/200\n",
      "241/241 [==============================] - 0s - loss: 0.1532 - acc: 0.9876 - val_loss: 0.3309 - val_acc: 0.8333\n",
      "Epoch 36/200\n",
      "241/241 [==============================] - 0s - loss: 0.1488 - acc: 0.9876 - val_loss: 0.3201 - val_acc: 0.8333\n",
      "Epoch 37/200\n",
      "241/241 [==============================] - 0s - loss: 0.1426 - acc: 0.9876 - val_loss: 0.3224 - val_acc: 0.8667\n",
      "Epoch 38/200\n",
      "241/241 [==============================] - 0s - loss: 0.1409 - acc: 0.9876 - val_loss: 0.3187 - val_acc: 0.8667\n",
      "Epoch 39/200\n",
      "241/241 [==============================] - 0s - loss: 0.1351 - acc: 0.9876 - val_loss: 0.3226 - val_acc: 0.8500\n",
      "Epoch 40/200\n",
      "241/241 [==============================] - 0s - loss: 0.1313 - acc: 0.9917 - val_loss: 0.3206 - val_acc: 0.8500\n",
      "Epoch 41/200\n",
      "241/241 [==============================] - 0s - loss: 0.1312 - acc: 0.9876 - val_loss: 0.3194 - val_acc: 0.8667\n",
      "Epoch 42/200\n",
      "241/241 [==============================] - 0s - loss: 0.1238 - acc: 0.9959 - val_loss: 0.3180 - val_acc: 0.8500\n",
      "Epoch 43/200\n",
      "241/241 [==============================] - 0s - loss: 0.1219 - acc: 0.9917 - val_loss: 0.3131 - val_acc: 0.8500\n",
      "Epoch 44/200\n",
      "241/241 [==============================] - 0s - loss: 0.1177 - acc: 0.9917 - val_loss: 0.3175 - val_acc: 0.8667\n",
      "Epoch 45/200\n",
      "241/241 [==============================] - 0s - loss: 0.1155 - acc: 0.9959 - val_loss: 0.3174 - val_acc: 0.8833\n",
      "Epoch 46/200\n",
      "241/241 [==============================] - 0s - loss: 0.1117 - acc: 0.9959 - val_loss: 0.3185 - val_acc: 0.8833\n",
      "Epoch 47/200\n",
      "241/241 [==============================] - 0s - loss: 0.1090 - acc: 0.9959 - val_loss: 0.3116 - val_acc: 0.8833\n",
      "Epoch 48/200\n",
      "241/241 [==============================] - 0s - loss: 0.1062 - acc: 0.9959 - val_loss: 0.3035 - val_acc: 0.8667\n",
      "Epoch 49/200\n",
      "241/241 [==============================] - 0s - loss: 0.1037 - acc: 0.9959 - val_loss: 0.3078 - val_acc: 0.8667\n",
      "Epoch 50/200\n",
      "241/241 [==============================] - 0s - loss: 0.1010 - acc: 0.9959 - val_loss: 0.3102 - val_acc: 0.8833\n",
      "Epoch 51/200\n",
      "241/241 [==============================] - 0s - loss: 0.0992 - acc: 0.9959 - val_loss: 0.3259 - val_acc: 0.8833\n",
      "Epoch 52/200\n",
      "241/241 [==============================] - 0s - loss: 0.0969 - acc: 0.9959 - val_loss: 0.3063 - val_acc: 0.8833\n",
      "Epoch 53/200\n",
      "241/241 [==============================] - 0s - loss: 0.0939 - acc: 1.0000 - val_loss: 0.2988 - val_acc: 0.8667\n",
      "Epoch 54/200\n",
      "241/241 [==============================] - 0s - loss: 0.0919 - acc: 1.0000 - val_loss: 0.3125 - val_acc: 0.8833\n",
      "Epoch 55/200\n",
      "241/241 [==============================] - 0s - loss: 0.0896 - acc: 0.9959 - val_loss: 0.3112 - val_acc: 0.8833\n",
      "Epoch 56/200\n",
      "241/241 [==============================] - 0s - loss: 0.0882 - acc: 0.9959 - val_loss: 0.3068 - val_acc: 0.8833\n",
      "Epoch 57/200\n",
      "241/241 [==============================] - 0s - loss: 0.0858 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.8833\n",
      "Epoch 58/200\n",
      "241/241 [==============================] - 0s - loss: 0.0847 - acc: 1.0000 - val_loss: 0.3089 - val_acc: 0.8833\n",
      "Epoch 59/200\n",
      "241/241 [==============================] - 0s - loss: 0.0821 - acc: 1.0000 - val_loss: 0.3200 - val_acc: 0.8833\n",
      "Epoch 60/200\n",
      "241/241 [==============================] - 0s - loss: 0.0815 - acc: 0.9959 - val_loss: 0.3039 - val_acc: 0.8833\n",
      "Epoch 61/200\n",
      "241/241 [==============================] - 0s - loss: 0.0789 - acc: 1.0000 - val_loss: 0.3032 - val_acc: 0.8833\n",
      "Epoch 62/200\n",
      "241/241 [==============================] - 0s - loss: 0.0773 - acc: 1.0000 - val_loss: 0.3046 - val_acc: 0.8833\n",
      "Epoch 63/200\n",
      "241/241 [==============================] - 0s - loss: 0.0754 - acc: 1.0000 - val_loss: 0.3006 - val_acc: 0.8833\n",
      "Epoch 64/200\n",
      "241/241 [==============================] - 0s - loss: 0.0744 - acc: 1.0000 - val_loss: 0.3059 - val_acc: 0.8833\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 0s - loss: 0.0720 - acc: 1.0000 - val_loss: 0.3058 - val_acc: 0.8833\n",
      "Epoch 66/200\n",
      "241/241 [==============================] - 0s - loss: 0.0708 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.8833\n",
      "Epoch 67/200\n",
      "241/241 [==============================] - 0s - loss: 0.0695 - acc: 1.0000 - val_loss: 0.2998 - val_acc: 0.8833\n",
      "Epoch 68/200\n",
      "241/241 [==============================] - 0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.3033 - val_acc: 0.8833\n",
      "Epoch 69/200\n",
      "241/241 [==============================] - 0s - loss: 0.0667 - acc: 1.0000 - val_loss: 0.2963 - val_acc: 0.8833\n",
      "Epoch 70/200\n",
      "241/241 [==============================] - 0s - loss: 0.0654 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.8833\n",
      "Epoch 71/200\n",
      "241/241 [==============================] - 0s - loss: 0.0645 - acc: 1.0000 - val_loss: 0.3091 - val_acc: 0.8833\n",
      "Epoch 72/200\n",
      "241/241 [==============================] - 0s - loss: 0.0632 - acc: 1.0000 - val_loss: 0.3049 - val_acc: 0.8833\n",
      "Epoch 73/200\n",
      "241/241 [==============================] - 0s - loss: 0.0619 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 0.8833\n",
      "Epoch 74/200\n",
      "241/241 [==============================] - 0s - loss: 0.0609 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.8833\n",
      "Epoch 75/200\n",
      "241/241 [==============================] - 0s - loss: 0.0604 - acc: 1.0000 - val_loss: 0.2958 - val_acc: 0.8833\n",
      "Epoch 76/200\n",
      "241/241 [==============================] - 0s - loss: 0.0589 - acc: 1.0000 - val_loss: 0.3199 - val_acc: 0.8833\n",
      "Epoch 77/200\n",
      "241/241 [==============================] - 0s - loss: 0.0582 - acc: 1.0000 - val_loss: 0.3076 - val_acc: 0.8833\n",
      "Epoch 78/200\n",
      "241/241 [==============================] - 0s - loss: 0.0567 - acc: 1.0000 - val_loss: 0.2903 - val_acc: 0.8833\n",
      "Epoch 79/200\n",
      "241/241 [==============================] - 0s - loss: 0.0556 - acc: 1.0000 - val_loss: 0.2940 - val_acc: 0.8833\n",
      "Epoch 80/200\n",
      "241/241 [==============================] - 0s - loss: 0.0548 - acc: 1.0000 - val_loss: 0.3011 - val_acc: 0.8833\n",
      "Epoch 81/200\n",
      "241/241 [==============================] - 0s - loss: 0.0536 - acc: 1.0000 - val_loss: 0.3038 - val_acc: 0.8833\n",
      "Epoch 82/200\n",
      "241/241 [==============================] - 0s - loss: 0.0525 - acc: 1.0000 - val_loss: 0.2950 - val_acc: 0.8833\n",
      "Epoch 83/200\n",
      "241/241 [==============================] - 0s - loss: 0.0520 - acc: 1.0000 - val_loss: 0.2894 - val_acc: 0.8833\n",
      "Epoch 84/200\n",
      "241/241 [==============================] - 0s - loss: 0.0511 - acc: 1.0000 - val_loss: 0.2971 - val_acc: 0.8833\n",
      "Epoch 85/200\n",
      "241/241 [==============================] - 0s - loss: 0.0505 - acc: 1.0000 - val_loss: 0.3107 - val_acc: 0.8833\n",
      "Epoch 86/200\n",
      "241/241 [==============================] - 0s - loss: 0.0495 - acc: 1.0000 - val_loss: 0.2984 - val_acc: 0.8833\n",
      "Epoch 87/200\n",
      "241/241 [==============================] - 0s - loss: 0.0487 - acc: 1.0000 - val_loss: 0.2878 - val_acc: 0.8833\n",
      "Epoch 88/200\n",
      "241/241 [==============================] - 0s - loss: 0.0478 - acc: 1.0000 - val_loss: 0.3001 - val_acc: 0.8833\n",
      "Epoch 89/200\n",
      "241/241 [==============================] - 0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.2985 - val_acc: 0.8833\n",
      "Epoch 90/200\n",
      "241/241 [==============================] - 0s - loss: 0.0464 - acc: 1.0000 - val_loss: 0.2978 - val_acc: 0.8833\n",
      "Epoch 91/200\n",
      "241/241 [==============================] - 0s - loss: 0.0457 - acc: 1.0000 - val_loss: 0.2887 - val_acc: 0.8833\n",
      "Epoch 92/200\n",
      "241/241 [==============================] - 0s - loss: 0.0449 - acc: 1.0000 - val_loss: 0.2928 - val_acc: 0.8833\n",
      "Epoch 93/200\n",
      "241/241 [==============================] - 0s - loss: 0.0442 - acc: 1.0000 - val_loss: 0.3060 - val_acc: 0.8833\n",
      "Epoch 94/200\n",
      "241/241 [==============================] - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.3009 - val_acc: 0.8833\n",
      "Epoch 95/200\n",
      "241/241 [==============================] - 0s - loss: 0.0426 - acc: 1.0000 - val_loss: 0.2886 - val_acc: 0.8833\n",
      "Epoch 96/200\n",
      "241/241 [==============================] - 0s - loss: 0.0422 - acc: 1.0000 - val_loss: 0.2886 - val_acc: 0.8833\n",
      "Epoch 97/200\n",
      "241/241 [==============================] - 0s - loss: 0.0424 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 0.8833\n",
      "Epoch 98/200\n",
      "241/241 [==============================] - 0s - loss: 0.0414 - acc: 1.0000 - val_loss: 0.2999 - val_acc: 0.8833\n",
      "Epoch 99/200\n",
      "241/241 [==============================] - 0s - loss: 0.0405 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.8833\n",
      "Epoch 100/200\n",
      "241/241 [==============================] - 0s - loss: 0.0397 - acc: 1.0000 - val_loss: 0.2942 - val_acc: 0.8833\n",
      "Epoch 101/200\n",
      "241/241 [==============================] - 0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 0.8833\n",
      "Epoch 102/200\n",
      "241/241 [==============================] - 0s - loss: 0.0388 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 0.8833\n",
      "Epoch 103/200\n",
      "241/241 [==============================] - 0s - loss: 0.0379 - acc: 1.0000 - val_loss: 0.2944 - val_acc: 0.8833\n",
      "Epoch 104/200\n",
      "241/241 [==============================] - 0s - loss: 0.0376 - acc: 1.0000 - val_loss: 0.2894 - val_acc: 0.8833\n",
      "Epoch 105/200\n",
      "241/241 [==============================] - 0s - loss: 0.0370 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 0.8833\n",
      "Epoch 106/200\n",
      "241/241 [==============================] - 0s - loss: 0.0364 - acc: 1.0000 - val_loss: 0.2980 - val_acc: 0.8833\n",
      "Epoch 107/200\n",
      "241/241 [==============================] - 0s - loss: 0.0360 - acc: 1.0000 - val_loss: 0.2942 - val_acc: 0.8833\n",
      "Epoch 108/200\n",
      "241/241 [==============================] - 0s - loss: 0.0356 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 0.8833\n",
      "Epoch 109/200\n",
      "241/241 [==============================] - 0s - loss: 0.0354 - acc: 1.0000 - val_loss: 0.2954 - val_acc: 0.8667\n",
      "Epoch 110/200\n",
      "241/241 [==============================] - 0s - loss: 0.0347 - acc: 1.0000 - val_loss: 0.2900 - val_acc: 0.8833\n",
      "Epoch 111/200\n",
      "241/241 [==============================] - 0s - loss: 0.0347 - acc: 1.0000 - val_loss: 0.2952 - val_acc: 0.8833\n",
      "Epoch 112/200\n",
      "241/241 [==============================] - 0s - loss: 0.0339 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 0.8833\n",
      "Epoch 113/200\n",
      "241/241 [==============================] - 0s - loss: 0.0335 - acc: 1.0000 - val_loss: 0.3033 - val_acc: 0.8667\n",
      "Epoch 114/200\n",
      "241/241 [==============================] - 0s - loss: 0.0332 - acc: 1.0000 - val_loss: 0.2883 - val_acc: 0.8833\n",
      "Epoch 115/200\n",
      "241/241 [==============================] - 0s - loss: 0.0324 - acc: 1.0000 - val_loss: 0.2970 - val_acc: 0.8833\n",
      "Epoch 116/200\n",
      "241/241 [==============================] - 0s - loss: 0.0321 - acc: 1.0000 - val_loss: 0.2935 - val_acc: 0.8833\n",
      "Epoch 117/200\n",
      "241/241 [==============================] - 0s - loss: 0.0316 - acc: 1.0000 - val_loss: 0.2941 - val_acc: 0.8833\n",
      "Epoch 118/200\n",
      "241/241 [==============================] - 0s - loss: 0.0312 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 0.8833\n",
      "Epoch 119/200\n",
      "241/241 [==============================] - 0s - loss: 0.0306 - acc: 1.0000 - val_loss: 0.2978 - val_acc: 0.8833\n",
      "Epoch 120/200\n",
      "241/241 [==============================] - 0s - loss: 0.0305 - acc: 1.0000 - val_loss: 0.2939 - val_acc: 0.8833\n",
      "Epoch 121/200\n",
      "241/241 [==============================] - 0s - loss: 0.0299 - acc: 1.0000 - val_loss: 0.2932 - val_acc: 0.8833\n",
      "Epoch 122/200\n",
      "241/241 [==============================] - 0s - loss: 0.0297 - acc: 1.0000 - val_loss: 0.2964 - val_acc: 0.8667\n",
      "Epoch 123/200\n",
      "241/241 [==============================] - 0s - loss: 0.0294 - acc: 1.0000 - val_loss: 0.2986 - val_acc: 0.8833\n",
      "Epoch 124/200\n",
      "241/241 [==============================] - 0s - loss: 0.0289 - acc: 1.0000 - val_loss: 0.2910 - val_acc: 0.8833\n",
      "Epoch 125/200\n",
      "241/241 [==============================] - 0s - loss: 0.0285 - acc: 1.0000 - val_loss: 0.2935 - val_acc: 0.8833\n",
      "Epoch 126/200\n",
      "241/241 [==============================] - 0s - loss: 0.0282 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8833\n",
      "Epoch 127/200\n",
      "241/241 [==============================] - 0s - loss: 0.0280 - acc: 1.0000 - val_loss: 0.3001 - val_acc: 0.8833\n",
      "Epoch 128/200\n",
      "241/241 [==============================] - 0s - loss: 0.0275 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 0.8833\n",
      "Epoch 129/200\n",
      "241/241 [==============================] - 0s - loss: 0.0272 - acc: 1.0000 - val_loss: 0.2918 - val_acc: 0.8833\n",
      "Epoch 130/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 0s - loss: 0.0269 - acc: 1.0000 - val_loss: 0.2908 - val_acc: 0.8833\n",
      "Epoch 131/200\n",
      "241/241 [==============================] - 0s - loss: 0.0266 - acc: 1.0000 - val_loss: 0.2934 - val_acc: 0.8833\n",
      "Epoch 132/200\n",
      "241/241 [==============================] - 0s - loss: 0.0263 - acc: 1.0000 - val_loss: 0.2956 - val_acc: 0.8833\n",
      "Epoch 133/200\n",
      "241/241 [==============================] - 0s - loss: 0.0262 - acc: 1.0000 - val_loss: 0.2943 - val_acc: 0.8833\n",
      "Epoch 134/200\n",
      "241/241 [==============================] - 0s - loss: 0.0256 - acc: 1.0000 - val_loss: 0.2928 - val_acc: 0.8833\n",
      "Epoch 135/200\n",
      "241/241 [==============================] - 0s - loss: 0.0255 - acc: 1.0000 - val_loss: 0.2925 - val_acc: 0.8833\n",
      "Epoch 136/200\n",
      "241/241 [==============================] - 0s - loss: 0.0251 - acc: 1.0000 - val_loss: 0.2947 - val_acc: 0.8833\n",
      "Epoch 137/200\n",
      "241/241 [==============================] - 0s - loss: 0.0248 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8833\n",
      "Epoch 138/200\n",
      "241/241 [==============================] - 0s - loss: 0.0246 - acc: 1.0000 - val_loss: 0.2945 - val_acc: 0.8833\n",
      "Epoch 139/200\n",
      "241/241 [==============================] - 0s - loss: 0.0244 - acc: 1.0000 - val_loss: 0.2990 - val_acc: 0.8833\n",
      "Epoch 140/200\n",
      "241/241 [==============================] - 0s - loss: 0.0241 - acc: 1.0000 - val_loss: 0.2913 - val_acc: 0.8833\n",
      "Epoch 141/200\n",
      "241/241 [==============================] - 0s - loss: 0.0238 - acc: 1.0000 - val_loss: 0.2868 - val_acc: 0.9000\n",
      "Epoch 142/200\n",
      "241/241 [==============================] - 0s - loss: 0.0236 - acc: 1.0000 - val_loss: 0.2888 - val_acc: 0.9000\n",
      "Epoch 143/200\n",
      "241/241 [==============================] - 0s - loss: 0.0233 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.8833\n",
      "Epoch 144/200\n",
      "241/241 [==============================] - 0s - loss: 0.0231 - acc: 1.0000 - val_loss: 0.3045 - val_acc: 0.8667\n",
      "Epoch 145/200\n",
      "241/241 [==============================] - 0s - loss: 0.0230 - acc: 1.0000 - val_loss: 0.2981 - val_acc: 0.8667\n",
      "Epoch 146/200\n",
      "241/241 [==============================] - 0s - loss: 0.0227 - acc: 1.0000 - val_loss: 0.2899 - val_acc: 0.9000\n",
      "Epoch 147/200\n",
      "241/241 [==============================] - 0s - loss: 0.0223 - acc: 1.0000 - val_loss: 0.2919 - val_acc: 0.8833\n",
      "Epoch 148/200\n",
      "241/241 [==============================] - 0s - loss: 0.0221 - acc: 1.0000 - val_loss: 0.2985 - val_acc: 0.8833\n",
      "Epoch 149/200\n",
      "241/241 [==============================] - 0s - loss: 0.0220 - acc: 1.0000 - val_loss: 0.3023 - val_acc: 0.8833\n",
      "Epoch 150/200\n",
      "241/241 [==============================] - 0s - loss: 0.0218 - acc: 1.0000 - val_loss: 0.2923 - val_acc: 0.8667\n",
      "Epoch 151/200\n",
      "241/241 [==============================] - 0s - loss: 0.0215 - acc: 1.0000 - val_loss: 0.2901 - val_acc: 0.8833\n",
      "Epoch 152/200\n",
      "241/241 [==============================] - 0s - loss: 0.0212 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 0.8833\n",
      "Epoch 153/200\n",
      "241/241 [==============================] - 0s - loss: 0.0210 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8833\n",
      "Epoch 154/200\n",
      "241/241 [==============================] - 0s - loss: 0.0208 - acc: 1.0000 - val_loss: 0.2936 - val_acc: 0.8833\n",
      "Epoch 155/200\n",
      "241/241 [==============================] - 0s - loss: 0.0206 - acc: 1.0000 - val_loss: 0.2938 - val_acc: 0.8833\n",
      "Epoch 156/200\n",
      "241/241 [==============================] - 0s - loss: 0.0204 - acc: 1.0000 - val_loss: 0.2934 - val_acc: 0.8667\n",
      "Epoch 157/200\n",
      "241/241 [==============================] - 0s - loss: 0.0201 - acc: 1.0000 - val_loss: 0.2930 - val_acc: 0.8667\n",
      "Epoch 158/200\n",
      "241/241 [==============================] - 0s - loss: 0.0201 - acc: 1.0000 - val_loss: 0.2941 - val_acc: 0.8667\n",
      "Epoch 159/200\n",
      "241/241 [==============================] - 0s - loss: 0.0199 - acc: 1.0000 - val_loss: 0.2928 - val_acc: 0.8833\n",
      "Epoch 160/200\n",
      "241/241 [==============================] - 0s - loss: 0.0196 - acc: 1.0000 - val_loss: 0.2956 - val_acc: 0.8667\n",
      "Epoch 161/200\n",
      "241/241 [==============================] - 0s - loss: 0.0196 - acc: 1.0000 - val_loss: 0.2959 - val_acc: 0.8667\n",
      "Epoch 162/200\n",
      "241/241 [==============================] - 0s - loss: 0.0194 - acc: 1.0000 - val_loss: 0.2877 - val_acc: 0.9000\n",
      "Epoch 163/200\n",
      "241/241 [==============================] - 0s - loss: 0.0192 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8833\n",
      "Epoch 164/200\n",
      "241/241 [==============================] - 0s - loss: 0.0189 - acc: 1.0000 - val_loss: 0.2937 - val_acc: 0.8667\n",
      "Epoch 165/200\n",
      "241/241 [==============================] - 0s - loss: 0.0187 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8667\n",
      "Epoch 166/200\n",
      "241/241 [==============================] - 0s - loss: 0.0185 - acc: 1.0000 - val_loss: 0.2982 - val_acc: 0.8667\n",
      "Epoch 167/200\n",
      "241/241 [==============================] - 0s - loss: 0.0183 - acc: 1.0000 - val_loss: 0.2965 - val_acc: 0.8667\n",
      "Epoch 168/200\n",
      "241/241 [==============================] - 0s - loss: 0.0182 - acc: 1.0000 - val_loss: 0.2959 - val_acc: 0.8667\n",
      "Epoch 169/200\n",
      "241/241 [==============================] - 0s - loss: 0.0180 - acc: 1.0000 - val_loss: 0.2967 - val_acc: 0.8667\n",
      "Epoch 170/200\n",
      "241/241 [==============================] - 0s - loss: 0.0179 - acc: 1.0000 - val_loss: 0.2960 - val_acc: 0.8667\n",
      "Epoch 171/200\n",
      "241/241 [==============================] - 0s - loss: 0.0177 - acc: 1.0000 - val_loss: 0.3014 - val_acc: 0.8667\n",
      "Epoch 172/200\n",
      "241/241 [==============================] - 0s - loss: 0.0175 - acc: 1.0000 - val_loss: 0.2989 - val_acc: 0.8667\n",
      "Epoch 173/200\n",
      "241/241 [==============================] - 0s - loss: 0.0174 - acc: 1.0000 - val_loss: 0.2946 - val_acc: 0.8667\n",
      "Epoch 174/200\n",
      "241/241 [==============================] - 0s - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2873 - val_acc: 0.9000\n",
      "Epoch 175/200\n",
      "241/241 [==============================] - 0s - loss: 0.0172 - acc: 1.0000 - val_loss: 0.2903 - val_acc: 0.8833\n",
      "Epoch 176/200\n",
      "241/241 [==============================] - 0s - loss: 0.0169 - acc: 1.0000 - val_loss: 0.2907 - val_acc: 0.8833\n",
      "Epoch 177/200\n",
      "241/241 [==============================] - 0s - loss: 0.0168 - acc: 1.0000 - val_loss: 0.2927 - val_acc: 0.8667\n",
      "Epoch 178/200\n",
      "241/241 [==============================] - 0s - loss: 0.0167 - acc: 1.0000 - val_loss: 0.3007 - val_acc: 0.8667\n",
      "Epoch 179/200\n",
      "241/241 [==============================] - 0s - loss: 0.0165 - acc: 1.0000 - val_loss: 0.2989 - val_acc: 0.8667\n",
      "Epoch 180/200\n",
      "241/241 [==============================] - 0s - loss: 0.0163 - acc: 1.0000 - val_loss: 0.2952 - val_acc: 0.8667\n",
      "Epoch 181/200\n",
      "241/241 [==============================] - 0s - loss: 0.0162 - acc: 1.0000 - val_loss: 0.2929 - val_acc: 0.8667\n",
      "Epoch 182/200\n",
      "241/241 [==============================] - 0s - loss: 0.0161 - acc: 1.0000 - val_loss: 0.2920 - val_acc: 0.8833\n",
      "Epoch 183/200\n",
      "241/241 [==============================] - 0s - loss: 0.0159 - acc: 1.0000 - val_loss: 0.2961 - val_acc: 0.8667\n",
      "Epoch 184/200\n",
      "241/241 [==============================] - 0s - loss: 0.0158 - acc: 1.0000 - val_loss: 0.2991 - val_acc: 0.8667\n",
      "Epoch 185/200\n",
      "241/241 [==============================] - 0s - loss: 0.0157 - acc: 1.0000 - val_loss: 0.3014 - val_acc: 0.8667\n",
      "Epoch 186/200\n",
      "241/241 [==============================] - 0s - loss: 0.0155 - acc: 1.0000 - val_loss: 0.3001 - val_acc: 0.8667\n",
      "Epoch 187/200\n",
      "241/241 [==============================] - 0s - loss: 0.0153 - acc: 1.0000 - val_loss: 0.2926 - val_acc: 0.8833\n",
      "Epoch 188/200\n",
      "241/241 [==============================] - 0s - loss: 0.0154 - acc: 1.0000 - val_loss: 0.2854 - val_acc: 0.8833\n",
      "Epoch 189/200\n",
      "241/241 [==============================] - 0s - loss: 0.0152 - acc: 1.0000 - val_loss: 0.2889 - val_acc: 0.8833\n",
      "Epoch 190/200\n",
      "241/241 [==============================] - 0s - loss: 0.0150 - acc: 1.0000 - val_loss: 0.2979 - val_acc: 0.8667\n",
      "Epoch 191/200\n",
      "241/241 [==============================] - 0s - loss: 0.0149 - acc: 1.0000 - val_loss: 0.3026 - val_acc: 0.8667\n",
      "Epoch 192/200\n",
      "241/241 [==============================] - 0s - loss: 0.0148 - acc: 1.0000 - val_loss: 0.3029 - val_acc: 0.8667\n",
      "Epoch 193/200\n",
      "241/241 [==============================] - 0s - loss: 0.0146 - acc: 1.0000 - val_loss: 0.2949 - val_acc: 0.8667\n",
      "Epoch 194/200\n",
      "241/241 [==============================] - 0s - loss: 0.0145 - acc: 1.0000 - val_loss: 0.2920 - val_acc: 0.8833\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/241 [==============================] - 0s - loss: 0.0144 - acc: 1.0000 - val_loss: 0.2957 - val_acc: 0.8667\n",
      "Epoch 196/200\n",
      "241/241 [==============================] - 0s - loss: 0.0143 - acc: 1.0000 - val_loss: 0.2968 - val_acc: 0.8667\n",
      "Epoch 197/200\n",
      "241/241 [==============================] - 0s - loss: 0.0143 - acc: 1.0000 - val_loss: 0.3025 - val_acc: 0.8667\n",
      "Epoch 198/200\n",
      "241/241 [==============================] - 0s - loss: 0.0141 - acc: 1.0000 - val_loss: 0.2994 - val_acc: 0.8667\n",
      "Epoch 199/200\n",
      "241/241 [==============================] - 0s - loss: 0.0139 - acc: 1.0000 - val_loss: 0.2953 - val_acc: 0.8667\n",
      "Epoch 200/200\n",
      "241/241 [==============================] - 0s - loss: 0.0138 - acc: 1.0000 - val_loss: 0.2939 - val_acc: 0.8667\n",
      "[0] Pre-trained acurracy (top_model): 0.8667\n",
      "[0] Fine-tuning the model ...\n",
      "Train on 241 samples, validate on 60 samples\n",
      "Epoch 1/200\n",
      "241/241 [==============================] - 12s - loss: 2.4024 - acc: 0.5187 - val_loss: 0.2316 - val_acc: 0.9167\n",
      "Epoch 2/200\n",
      "241/241 [==============================] - 2s - loss: 1.1465 - acc: 0.6805 - val_loss: 0.3508 - val_acc: 0.8667\n",
      "Epoch 3/200\n",
      "241/241 [==============================] - 2s - loss: 0.2461 - acc: 0.9336 - val_loss: 0.7010 - val_acc: 0.7833\n",
      "Epoch 4/200\n",
      "241/241 [==============================] - 2s - loss: 0.0666 - acc: 0.9876 - val_loss: 0.9928 - val_acc: 0.7667\n",
      "Epoch 5/200\n",
      "241/241 [==============================] - 2s - loss: 0.0285 - acc: 0.9959 - val_loss: 1.1351 - val_acc: 0.7333\n",
      "Epoch 6/200\n",
      "241/241 [==============================] - 2s - loss: 0.0129 - acc: 1.0000 - val_loss: 1.1634 - val_acc: 0.7333\n",
      "Epoch 7/200\n",
      "241/241 [==============================] - 2s - loss: 0.0098 - acc: 1.0000 - val_loss: 1.1198 - val_acc: 0.7333\n",
      "Epoch 8/200\n",
      "241/241 [==============================] - 2s - loss: 0.0104 - acc: 0.9959 - val_loss: 1.0356 - val_acc: 0.7333\n",
      "Epoch 9/200\n",
      "241/241 [==============================] - 2s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.9345 - val_acc: 0.7333\n",
      "Epoch 10/200\n",
      "241/241 [==============================] - 2s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.8319 - val_acc: 0.7667\n",
      "Epoch 11/200\n",
      "241/241 [==============================] - 2s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.7390 - val_acc: 0.8000\n",
      "Epoch 12/200\n",
      "241/241 [==============================] - 2s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.6563 - val_acc: 0.8000\n",
      "Epoch 13/200\n",
      "241/241 [==============================] - 2s - loss: 0.0043 - acc: 1.0000 - val_loss: 0.5833 - val_acc: 0.8167\n",
      "Epoch 14/200\n",
      "241/241 [==============================] - 2s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5197 - val_acc: 0.8167\n",
      "Epoch 15/200\n",
      "241/241 [==============================] - 2s - loss: 0.0045 - acc: 1.0000 - val_loss: 0.4613 - val_acc: 0.8167\n",
      "Epoch 16/200\n",
      "241/241 [==============================] - 2s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.8333\n",
      "Epoch 17/200\n",
      "241/241 [==============================] - 2s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3711 - val_acc: 0.8833\n",
      "Epoch 18/200\n",
      "241/241 [==============================] - 2s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3370 - val_acc: 0.9000\n",
      "Epoch 19/200\n",
      "241/241 [==============================] - 2s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.3136 - val_acc: 0.9000\n",
      "Epoch 20/200\n",
      "241/241 [==============================] - 3s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.2967 - val_acc: 0.9333\n",
      "Epoch 21/200\n",
      "241/241 [==============================] - 2s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2832 - val_acc: 0.9333\n",
      "Epoch 22/200\n",
      "241/241 [==============================] - 3s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.2741 - val_acc: 0.9500\n",
      "Epoch 23/200\n",
      "241/241 [==============================] - 2s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2709 - val_acc: 0.9500\n",
      "Epoch 24/200\n",
      "241/241 [==============================] - 2s - loss: 0.0026 - acc: 1.0000 - val_loss: 0.2702 - val_acc: 0.9500\n",
      "Epoch 25/200\n",
      "241/241 [==============================] - 2s - loss: 0.0029 - acc: 1.0000 - val_loss: 0.2735 - val_acc: 0.9167\n",
      "Epoch 26/200\n",
      "241/241 [==============================] - 2s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2817 - val_acc: 0.9000\n",
      "Epoch 27/200\n",
      "241/241 [==============================] - 2s - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2915 - val_acc: 0.9000\n",
      "Epoch 28/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3054 - val_acc: 0.8833\n",
      "Epoch 29/200\n",
      "241/241 [==============================] - 2s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3211 - val_acc: 0.8833\n",
      "Epoch 30/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3362 - val_acc: 0.8667\n",
      "Epoch 31/200\n",
      "241/241 [==============================] - 2s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3517 - val_acc: 0.8667\n",
      "Epoch 32/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.3694 - val_acc: 0.8667\n",
      "Epoch 33/200\n",
      "241/241 [==============================] - 2s - loss: 9.7737e-04 - acc: 1.0000 - val_loss: 0.3872 - val_acc: 0.8667\n",
      "Epoch 34/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4050 - val_acc: 0.8500\n",
      "Epoch 35/200\n",
      "241/241 [==============================] - 2s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.4220 - val_acc: 0.8500\n",
      "Epoch 36/200\n",
      "241/241 [==============================] - 2s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4368 - val_acc: 0.8500\n",
      "Epoch 37/200\n",
      "241/241 [==============================] - 2s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4493 - val_acc: 0.8500\n",
      "Epoch 38/200\n",
      "241/241 [==============================] - 2s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4628 - val_acc: 0.8500\n",
      "Epoch 39/200\n",
      "241/241 [==============================] - 2s - loss: 8.7502e-04 - acc: 1.0000 - val_loss: 0.4746 - val_acc: 0.8500\n",
      "Epoch 40/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.4847 - val_acc: 0.8500\n",
      "Epoch 41/200\n",
      "241/241 [==============================] - 2s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.4955 - val_acc: 0.8500\n",
      "Epoch 42/200\n",
      "241/241 [==============================] - 2s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.5068 - val_acc: 0.8500\n",
      "Epoch 43/200\n",
      "241/241 [==============================] - 2s - loss: 8.7104e-04 - acc: 1.0000 - val_loss: 0.5151 - val_acc: 0.8500\n",
      "Epoch 44/200\n",
      "241/241 [==============================] - 2s - loss: 9.3390e-04 - acc: 1.0000 - val_loss: 0.5228 - val_acc: 0.8500\n",
      "Epoch 45/200\n",
      "241/241 [==============================] - 2s - loss: 7.7959e-04 - acc: 1.0000 - val_loss: 0.5303 - val_acc: 0.8500\n",
      "Epoch 46/200\n",
      "241/241 [==============================] - 2s - loss: 7.0148e-04 - acc: 1.0000 - val_loss: 0.5385 - val_acc: 0.8500\n",
      "Epoch 47/200\n",
      "241/241 [==============================] - 2s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5473 - val_acc: 0.8500\n",
      "Epoch 48/200\n",
      "241/241 [==============================] - 2s - loss: 7.2674e-04 - acc: 1.0000 - val_loss: 0.5557 - val_acc: 0.8500\n",
      "Epoch 49/200\n",
      "241/241 [==============================] - 2s - loss: 7.4258e-04 - acc: 1.0000 - val_loss: 0.5642 - val_acc: 0.8500\n",
      "Epoch 50/200\n",
      "241/241 [==============================] - 2s - loss: 8.5872e-04 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 0.8500\n",
      "Epoch 51/200\n",
      "241/241 [==============================] - 2s - loss: 8.4859e-04 - acc: 1.0000 - val_loss: 0.5779 - val_acc: 0.8333\n",
      "Epoch 52/200\n",
      "241/241 [==============================] - 2s - loss: 6.0821e-04 - acc: 1.0000 - val_loss: 0.5847 - val_acc: 0.8167\n",
      "Epoch 53/200\n",
      "241/241 [==============================] - 2s - loss: 8.6381e-04 - acc: 1.0000 - val_loss: 0.5924 - val_acc: 0.8000\n",
      "Epoch 54/200\n",
      "128/241 [==============>...............] - ETA: 1s - loss: 5.5416e-04 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "for i in range(kfold):\n",
    "    train_indices = skfind[i][0]\n",
    "    test_indices = skfind[i][1]\n",
    "    X_train = X[train_indices]\n",
    "    resnet50features_train = resnet50features[train_indices]\n",
    "    Y_train = Y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    resnet50features_test = resnet50features[test_indices]\n",
    "    Y_test = Y[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "  \n",
    "    print(\"[%d] Pre-training the top model ...\" %(i))\n",
    "\n",
    "    top_model.set_weights(init_top_weights)\n",
    "\n",
    "    y_prob = top_model.predict(resnet50features_test, verbose=0)  # Testing\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    print(\"[%d] Initial acurracy (top_model): %.4f\" %(i,accuracy_score(y_test,y_pred)))\n",
    "    \n",
    "    start = time.time()\n",
    "    h = top_model.fit(resnet50features_train, Y_train, validation_data=(resnet50features_test, Y_test), epochs=num_epochs, batch_size=64, verbose=1, callbacks=callbacks_list)\n",
    "    end = time.time()\n",
    "    tophistory.append(h)\n",
    "    \n",
    "    pretrained_top_weights = top_model.get_weights()\n",
    "          \n",
    "    y_prob = top_model.predict(resnet50features_test, verbose=0)  # Testing\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    print(\"[%d] Pre-trained acurracy (top_model): %.4f\" %(i,accuracy_score(y_test,y_pred)))\n",
    "   \n",
    "    print(\"[%d] Fine-tuning the model ...\" %(i))\n",
    "\n",
    "    model.set_weights(init_weights)\n",
    "    model.layers[-1].set_weights(init_top_weights)\n",
    "    \n",
    "    #y_prob = model.predict(X_test, verbose=0)  # Testing\n",
    "    #y_pred = np.argmax(y_prob, axis=1)\n",
    "    #print(\"[%d] Initial acurracy (model): %.4f\" %(i,accuracy_score(y_test,y_pred)))\n",
    "    \n",
    "    model.layers[-1].set_weights(pretrained_top_weights)\n",
    "\n",
    "    #y_prob = model.predict(X_test, verbose=0)  # Testing\n",
    "    #y_pred = np.argmax(y_prob, axis=1)\n",
    "    #print(\"[%d] Pre-trained acurracy (model): %.4f\" %(i,accuracy_score(y_test,y_pred)))    \n",
    "    \n",
    "    start = time.time()\n",
    "    h = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=num_epochs, batch_size=64, verbose=1, callbacks=callbacks_list)\n",
    "    end = time.time()\n",
    "    history.append(h)\n",
    "        \n",
    "    y_prob = model.predict(X_test, verbose=0)  # Testing\n",
    "    y_pred = np.argmax(y_prob, axis=1)\n",
    "    print(\"[%d] Test acurracy: %.4f (%.4f s)\" %(i,accuracy_score(y_test,y_pred),end-start))\n",
    "    \n",
    "    cm = confusion_matrix(y_test,y_pred)  # Compute confusion matrix for this fold\n",
    "    conf_mat = conf_mat + cm  # Compute global confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing the average accuracy\n",
    "avg_acc = np.trace(conf_mat)/np.sum(conf_mat)\n",
    "print(\"Average acurracy: %.4f\" %(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_acc(history):\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(14, 6)\n",
    "    ax = plt.subplot()\n",
    "    #plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))\n",
    "    for i in range(len(history)):\n",
    "        color=next(colors)\n",
    "        plt.plot(history[i].history['acc'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)\n",
    "        plt.plot(history[i].history['val_acc'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0.0,1.0))\n",
    "    plt.legend()\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_acc(tophistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(14, 6)\n",
    "    ax = plt.subplot()\n",
    "    #plt.title('Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    colors = iter(colormap.gist_rainbow(np.linspace(0, 1, len(history))))\n",
    "    for i in range(len(history)):\n",
    "        color=next(colors)\n",
    "        plt.plot(history[i].history['loss'], label='Train '+str(i), color=color, linestyle = 'solid', linewidth=2.0)\n",
    "        plt.plot(history[i].history['val_loss'], label='Test '+str(i), color=color, linestyle = 'dotted', linewidth=2.0)\n",
    "    plt.legend()\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_loss(tophistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "print(\"Plotting the confusion matrix\")\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(4, 3)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(conf_mat, cbar=False, annot=True, square=True,\n",
    "                 fmt='.0f', annot_kws={'size': 14}, linewidth = 0.2, cmap = 'binary',\n",
    "                 yticklabels=list_fams, xticklabels=list_fams)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "print(\"Plotting the confusion matrix normalized\")\n",
    "conf_mat_norm = conf_mat/np.sum(conf_mat,axis=1)  # Normalizing the confusion matrix\n",
    "conf_mat_norm = np.around(conf_mat_norm,decimals=2)  # rounding to display in figure\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(4, 3)\n",
    "sns.set(font_scale=1.25)\n",
    "hm = sns.heatmap(conf_mat_norm, cbar=False, annot=True, square=True,\n",
    "                 fmt='.2f', annot_kws={'size': 14}, linewidth = 0.1, cmap = 'binary',\n",
    "                 yticklabels=list_fams, xticklabels=list_fams)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in tophistory:\n",
    "    print(h.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for h in history:\n",
    "    print(h.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_mean_acc(history):\n",
    "    train_scores = np.zeros((len(history),len(history[0].history['acc'])))\n",
    "    for fold in range(len(history)):\n",
    "        train_scores[fold] = history[fold].history['acc']\n",
    "    test_scores = np.zeros((len(history),len(history[0].history['val_acc'])))\n",
    "    for fold in range(len(history)):\n",
    "        test_scores[fold] = history[fold].history['val_acc']\n",
    "    epochs = np.linspace(0, len(history[0].history['acc']), len(history[0].history['acc']))\n",
    "    train_scores_mean = np.mean(train_scores, axis=0)\n",
    "    train_scores_std = np.std(train_scores, axis=0)\n",
    "    test_scores_mean = np.mean(test_scores, axis=0)\n",
    "    test_scores_std = np.std(test_scores, axis=0)\n",
    "    \n",
    "    figsize=(14, 6)\n",
    "    text_fontsize=\"medium\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=text_fontsize)\n",
    "    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.fill_between(epochs, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    ax.fill_between(epochs, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax.plot(epochs, train_scores_mean, '-', color=\"r\", linewidth=2.0, label=\"Train\")\n",
    "    ax.plot(epochs, test_scores_mean, '-', color=\"g\", linewidth=2.0, label=\"Test\")\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc=\"best\", fontsize=text_fontsize)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0.0,1.0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_acc(tophistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_acc(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_mean_loss(history):\n",
    "    train_scores = np.zeros((len(history),len(history[0].history['loss'])))\n",
    "    for fold in range(len(history)):\n",
    "        train_scores[fold] = history[fold].history['loss']\n",
    "    test_scores = np.zeros((len(history),len(history[0].history['val_loss'])))\n",
    "    for fold in range(len(history)):\n",
    "        test_scores[fold] = history[fold].history['val_loss']\n",
    "    epochs = np.linspace(0, len(history[0].history['loss']), len(history[0].history['loss']))\n",
    "    train_scores_mean = np.mean(train_scores, axis=0)\n",
    "    train_scores_std = np.std(train_scores, axis=0)\n",
    "    test_scores_mean = np.mean(test_scores, axis=0)\n",
    "    test_scores_std = np.std(test_scores, axis=0)\n",
    "    \n",
    "    figsize=(14, 6)\n",
    "    text_fontsize=\"medium\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=text_fontsize)\n",
    "    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.fill_between(epochs, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    ax.fill_between(epochs, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax.plot(epochs, train_scores_mean, '-', color=\"r\", linewidth=2.0, label=\"Train\")\n",
    "    ax.plot(epochs, test_scores_mean, '-', color=\"g\", linewidth=2.0, label=\"Test\")\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc=\"best\", fontsize=text_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_loss(tophistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tophist ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, h in enumerate(tophist):\n",
    "    print(\"[%d] Test accuracy: %.4f\" %(i,h['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, h in enumerate(hist):\n",
    "    print(\"[%d] Test accuracy: %.4f\" %(i,h['val_acc'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_mean_acc(history):\n",
    "    train_scores = np.zeros((len(history),len(history[0]['acc'])))\n",
    "    for fold in range(len(history)):\n",
    "        train_scores[fold] = history[fold]['acc']\n",
    "    test_scores = np.zeros((len(history),len(history[0]['val_acc'])))\n",
    "    for fold in range(len(history)):\n",
    "        test_scores[fold] = history[fold]['val_acc']\n",
    "    epochs = np.linspace(0, len(history[0]['acc']), len(history[0]['acc']))\n",
    "    train_scores_mean = np.mean(train_scores, axis=0)\n",
    "    train_scores_std = np.std(train_scores, axis=0)\n",
    "    test_scores_mean = np.mean(test_scores, axis=0)\n",
    "    test_scores_std = np.std(test_scores, axis=0)\n",
    "    \n",
    "    figsize=(14, 6)\n",
    "    text_fontsize=\"medium\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=text_fontsize)\n",
    "    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(0.1))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.fill_between(epochs, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    ax.fill_between(epochs, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax.plot(epochs, train_scores_mean, '-', color=\"r\", linewidth=2.0, label=\"Train\")\n",
    "    ax.plot(epochs, test_scores_mean, '-', color=\"g\", linewidth=2.0, label=\"Test\")\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc=\"best\", fontsize=text_fontsize)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,0.0,1.09))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_acc(tophist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_acc(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def plot_mean_loss(history):\n",
    "    train_scores = np.zeros((len(history),len(history[0]['loss'])))\n",
    "    for fold in range(len(history)):\n",
    "        train_scores[fold] = history[fold]['loss']\n",
    "    test_scores = np.zeros((len(history),len(history[0]['val_loss'])))\n",
    "    for fold in range(len(history)):\n",
    "        test_scores[fold] = history[fold]['val_loss']\n",
    "    epochs = np.linspace(0, len(history[0]['loss']), len(history[0]['loss']))\n",
    "    train_scores_mean = np.mean(train_scores, axis=0)\n",
    "    train_scores_std = np.std(train_scores, axis=0)\n",
    "    test_scores_mean = np.mean(test_scores, axis=0)\n",
    "    test_scores_std = np.std(test_scores, axis=0)\n",
    "    \n",
    "    figsize=(14, 6)\n",
    "    text_fontsize=\"medium\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=text_fontsize)\n",
    "    ax.set_ylabel(\"Score\", fontsize=text_fontsize)\n",
    "    ax.grid(True)\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(10))\n",
    "    ax.fill_between(epochs, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    ax.fill_between(epochs, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    ax.plot(epochs, train_scores_mean, '-', color=\"r\", linewidth=2.0, label=\"Train\")\n",
    "    ax.plot(epochs, test_scores_mean, '-', color=\"g\", linewidth=2.0, alpha=0.7, label=\"Test\")\n",
    "    ax.tick_params(labelsize=text_fontsize)\n",
    "    ax.legend(loc=\"best\", fontsize=text_fontsize)\n",
    "    x1,x2,y1,y2 = plt.axis()\n",
    "    plt.axis((x1,x2,-0.9,10.9))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_loss(tophist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_mean_loss(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
